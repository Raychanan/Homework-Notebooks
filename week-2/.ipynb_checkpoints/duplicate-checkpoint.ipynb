{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Counting Words & Phrases\n",
    "\n",
    "This week, we take text corpora that we have developed, spidered, scraped, and encoded, and we find and count words, simple and parsed phrases and explore the statistical properties of those counts (e.g., word frequency distributions). Initially, we model how to search corpora for keywords or phrases. Next, we examine the distributions of terms and phrases across a corpus, and the correlation between different words and phrase counts. In order to do this effectively, we coarsely disambiguate words based of part-of-speech (POS) tagging, and normalize them through stemming and lemmatization. Next we distinguish *important* words and phrase within the corpus, and image them with Wordls! Then we calculate word frequenceis, conditional frequences (the frequency of word *shock* conditional on the presence of word *awe*), and statistically significant collocations of lengths 2 through $n$. Finally, we calculate and visualize Differences (Divergences and Distances) between the word frequency distributions from two corpora. \n",
    "\n",
    "Then we shift to focus not simply on the *ideas* in a corpus, but also extracting precise *claims*. This process involves a sequential pipeline of classifying and structuring tokens from text, each of which generates potentially useful data for the content analyst. Steps in this process, which we examine in this notebook, include: 1) tagging words by their part of speech (POS) to reveal the linguistic role they play in the sentence (e.g., Verb, Noun, Adjective, etc.); 2) tagging words as named entities (NER) such as places or organizations; 3) structuring or \"parsing\" sentences into nested phrases that are local to, describe or depend on one another; and 4) extracting informational claims from those phrases.\n",
    "\n",
    "We will be introducing spaCy as a package for Computational Linguistics, and also walk you through how to use the Davies Corpora. \n",
    "\n",
    "While we wish to avoid using NLTK for heavy corpus analysis, some of its smaller functions can still be useful.\n",
    "\n",
    "Of course, for final projects that draw on these tools, we encourage you to install the software on your own machines or shared servers at the university (RCC, SSRC) in order to perform these operations on much more text. \n",
    "\n",
    "For this notebook we will be using the following packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "import lucem_illud #just in case, regularly update your lucem_illud with the following code: pip install git+git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "import requests #for http requests\n",
    "import pandas #gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import wordcloud #Makes word clouds\n",
    "import numpy as np #For divergences/distances\n",
    "import scipy #For divergences/distances\n",
    "import seaborn as sns #makes our plots look nicer\n",
    "import sklearn.manifold #For a manifold plot\n",
    "import json #For API responses\n",
    "import urllib.parse #For joining urls\n",
    "\n",
    "# comp-linguistics\n",
    "import spacy\n",
    "\n",
    "#Displays the graphs\n",
    "import graphviz #You also need to install the command line graphviz\n",
    "\n",
    "#These are from the standard library\n",
    "import os.path\n",
    "import zipfile\n",
    "import subprocess\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving exemplary corpora\n",
    "\n",
    "To get started we will need some examples. Let's start by downloading one of the corpuses from the Davies set purchased for this class and developed by a computational linguist at Brigham Young University. We can get a list of works available from the [corpora here](https://www.english-corpora.org/).\n",
    "\n",
    "In this notebook, I will be accessing the data locally - you also have access to this data through the Dropbox download link. The same code can be modified slighlty to access the rest of the corpora on RCC. \n",
    "There is a notebook describing this process in the same repository, as well as in the same directory on RCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = \"/Users/raychanan/Downloads/Movies\" \n",
    "# corpus_name = \"Movies\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be clear, your corpus_name should be different since you're not bhargav. Modify the corpus_name with your directory. This method extracts the text from the zip files and stroes the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadcorpus(corpus_name, corpus_style=\"text\"):\n",
    "    texts_raw = {}\n",
    "    for file in os.listdir(corpus_name + \"/\"):\n",
    "        if corpus_style in file:\n",
    "            print(file)\n",
    "            zfile = zipfile.ZipFile(corpus_name + \"/\" + file)\n",
    "            for file in zfile.namelist():\n",
    "                texts_raw[file] = []\n",
    "                with zfile.open(file) as f:\n",
    "                    for line in f:\n",
    "                        texts_raw[file].append(line)\n",
    "    return texts_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_13_idi.zip\n",
      "text_16_qll.zip\n",
      "text_32_ldf.zip\n",
      "text_19_gvc.zip\n",
      "text_05_nko.zip\n",
      "text_17_arp.zip\n",
      "text_01_ote.zip\n",
      "text_28_rfy.zip\n",
      "text_31_akv.zip\n",
      "text_22_etp.zip\n",
      "text_11_uoy.zip\n",
      "text_09_oii.zip\n",
      "text_06_jfy.zip\n",
      "text_14_lnc.zip\n",
      "text_08_loh.zip\n",
      "text_33_kje.zip\n",
      "text_30_wkp.zip\n",
      "text_07_oma.zip\n",
      "text_03_mnq.zip\n",
      "text_21_fqa.zip\n",
      "text_29_oye.zip\n",
      "text_27_fle.zip\n",
      "text_23_fmh.zip\n",
      "text_12_rcq.zip\n",
      "text_00_myn.zip\n",
      "text_10_aoy.zip\n",
      "text_04_mlq.zip\n",
      "text_20_cde.zip\n",
      "text_02_mqu.zip\n",
      "text_26_ngj.zip\n",
      "text_24_ywo.zip\n",
      "text_18_jfj.zip\n",
      "text_25_byg.zip\n",
      "text_15_guo.zip\n"
     ]
    }
   ],
   "source": [
    "movie_raw = loadcorpus(corpus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_raw['11.txt'][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems messy, but nothing we can't clean. This basic method replaces some of the issues with the formatting, and prints the errors if any for debugging. Let us clean one of the raw text files.\n",
    "\n",
    "We ignore the non-unicode data for convenience in our first pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_text(raw_texts):\n",
    "    clean_texts = []\n",
    "    for text in raw_texts:\n",
    "        try:\n",
    "            text = text.decode(\"utf-8\")\n",
    "            clean_text = text.replace(\" \\'m\", \"'m\").replace(\" \\'ll\", \"'ll\").replace(\" \\'re\", \"'re\").replace(\" \\'s\", \"'s\").replace(\" \\'re\", \"'re\").replace(\" n\\'t\", \"n't\").replace(\" \\'ve\", \"'ve\").replace(\" /'d\", \"'d\")\n",
    "            clean_texts.append(clean_text)\n",
    "        except AttributeError:\n",
    "            # print(\"ERROR CLEANING\")\n",
    "            # print(text)\n",
    "            continue\n",
    "        except UnicodeDecodeError:\n",
    "            # print(\"Unicode Error, Skip\")\n",
    "            continue\n",
    "    return clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_11 = clean_raw_text(movie_raw['11.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@@216680 Hey , I\\'m talking to you Give me 600 dollars You wish ! That\\'s all we\\'ve left And you still go to gamble Shut up I earn the money Even that you can\\'t take it for gamble Shut up What\\'re you doing ? Bastard , I\\'m gon na beat you You gambling pig , I\\'ll beat the shit out of you You bitch I\\'ll beat you You dare to hit me with something ! I\\'ll kill you All you know is gambling I\\'ll beat you What\\'re you doing ? Let go of me Stop You\\'ll kill Mom Mom , are you all right ? Don\\'t touch my money Dad , where\\'re you going ? Go away Dad , don\\'t go Get lost Dad , come back You take Marble with you The way she\\'s now Do you want to give me trouble ? I\\'m in more trouble than you I am a woman I\\'m a sailor You take care of Marble Just gamble a bit less is enough to raise the kid No way Raising her would be like raising others @ @ @ @ @ @ @ @ @ @ take care of her Marry her when she grows up And you tell her to be smart Don\\'t marry a gambler Hey , you want a quarrel ? No , never This is the last time You\\'re SO right Don\\'t rush ... Everyone will have a share Why so crowded ? What\\'re they doing ? Worshipping ancestors With all those many things ? Hey , Granny , Come on Come and take a bite The dishes are good this year Right Marble , have some I\\'ll go get my share of pork This is a leg-washing basin Hey , how can you say that ? Let\\'s go get a share of pork Your share ? You wish ! That\\'s only for boys Yeah , I have a wee-wee . Have you ? Aunty Chiang Lin , one more baby this year ? Yes You are the best in your family Hey , you think it\\'s my will I was compelled by my stupid husband Come on . Let\\'s worship first Eat some roast pork We have a lot of it Honey @ @ @ @ @ @ @ @ @ @ baby ? Yes . Only with Chinese blood What do you mean ? It means I give you a green hat Oh , that goes with my clothes then Eat , no more talk ! So rude . Take it home Get up , go We\\'re going back home Be quick Honey Hey , make a stance Hold it ... Master , every time we\\'ve to stand in a pose If you could teach us something else please Okay Shaolin Masters learn poses for years Hold it ... Okay . I\\'ll teach you something new Watch me See . Get to it Seeing Shanghai Beach in Mainland Here I\\'m still watching it everyday I\\'m so tired Tired ? Let\\'s go to bed for rest I hate Chau Yun Fa the most He didn\\'t marry when I\\'m in China As soon as I\\'m here , he got married You can\\'t come to Hong Kong if you don\\'t marry me You said you are big boss Having a big business Now that I\\'m in Hong Kong ... I\\'m still @ @ @ @ @ @ @ @ @ @ for myself Then I ought to feel much more sorry You can\\'t even lay an egg This year I can only get one share of pork And you\\'re blaming me for it ? Damn monkey ... What\\'s going on ? A letter for you Oh , it\\'s you Your monkey glared at me fiercely Will you open it for me ? Read it to me . I am quite illiterate Stop it . Don\\'t bother him Let me help you . Give me the letter Postman\\'s duty is to deliver and read letters He reads for Aunt Jiao How can you compare to her ? Aunty Jiao is 83 years old Hello , dubbing room Martin ? Martin is working Are you lying to me ? Why should I lie ? I know he\\'s free . Get him You listen yourself then Hello , what\\'s the noise ? Is someone being raped ? Martin , your telephone Hello What took you so long ? Recording is going on Don\\'t mess around Yes , I know . I won\\'t Honey , @ @ @ @ @ @ @ @ @ @ did you spend so much ? I was late for work ... so I took a taxi On your way back don\\'t forget to buy vegetables Okay And buy a pair of slippers for me Got it , honey I\\'ll hang up . Bye Wouldn\\'t marry if you know your wife is so bad ! Take pity for the rest of my life Hey , what\\'s on your neck ? Nothing You look piteous She\\'s the piteous one The vampire is here Grandma , grandma Someone is fighting Grandma , grandma Come over quickly What happened ? Someone is fighting Fighting ? Yes Where ? Over there ! You see it ? I spare you this time . No next time Sorry , let me kiss you Cut the bullshit . Go to bed I need to work tomorrow Where at ? That villa over there ! Wow , it looks beautiful Who lives there ? Snow White Kidding , there\\'s no Snow White Oh , murder That\\'s the pig scream Why\\'s the pig screaming so late at night ? Shorty\\'s @ @ @ @ @ @ @ @ @ @ late ? So the government won\\'t know But how could a pig\\'s scream be so terrible ? Why ? Why aren\\'t you tired ? Why aren\\'t you sleepy ? Why haven\\'t you gone to bed ? Why are you so nosy ? Go to sleep ! Here\\'s the kitchen Oil . Salt . Sauce . Vinegar . All there Go tidy up Ask me if you have any question Come dance with me Come on Come Dance I\\'ll teach you . Don\\'t be afraid Damn dog ! Damn it ! What\\'s such a big deal to be rich ? Pay for my pants Sorry . I don\\'t mean to be rude Be careful next time Miss , it seems I\\'ve seen you somewhere Of course you have I\\'ll be the Miss Hong Kong in next contest I have nine posters at the Ferry pier And I\\'ve been handing out my photographs all around Never seen one as rude as you Chang Jin This girl is great . She lives in your village ? Of course she\\'s @ @ @ @ @ @ @ @ @ @ And the ugly one\\'s only live in your village Miss , I want my BALL back We\\'re back from Holland Shall we make a friendship ? My dad said she got married Married ? No way Look at her curves ! She sure knows how to keep fit Maybe she never takes birth-control pills Hey , pals Be nice to a girl , huh ? What do you mean ? You think you\\'re a kungfu master ? Get the ball back Okay We\\'re friends . If you don\\'t give me face At least for my Dad\\'s sake ... let\\'s not make a fuss of this Are you alright ? He hits me . Beat him Stop Just playing . Stop What\\'re you on about ? I\\'ve stopped . Why don\\'t you stop ? I\\'ll let you go , damn kid Don\\'t let me see you again I\\'ll kill you next time Let\\'s go , go Waste of strength ! Post-boy . You think you\\'re good We\\'re taking pity on you Yeah Go , go Your @ @ @ @ @ @ @ @ @ @ Sorry to bother you again Please sign Where\\'s Wang ? He went to Kowloon on business Oh , you\\'re bleeding ? Who hurt you ? Let me help you Suppressing anger is not a good thing What ? Let me clean your wound No Okay , I won\\'t touch you I\\'ll give you some water , do it yourself How often do you deliver each day ? What are you drawing so late at night ? Go to bed . You have class tomorrow Go to sleep now Dad , I\\'m going to school now Go now . Don\\'t be late Okay , I know A shepherd drove sheep on the hill Watching farmers farming below ... Jin Bao I have twenty of them Give me one No way My uncle smuggled it from Mainland What is it ? How about I exchange one with a secret What secret ? Last night the White Man tortured his wife I don\\'t believe you He\\'s so coward to his wife he\\'ll never beat her Then will you give me ? What kind of @ @ @ @ @ @ @ @ @ @ shepherd laughed ... Master Good morning , teacher I\\'ve taught you ... to write yourselves a letter And learn to write your home address That \\'d help uncle Postman to deliver No , Mister Postman Oh yes , calling \\' uncle \\' is too old Yes Now come out to get your letter when I call you Wang Jia Leung Here Wang Shi Chung Here Shu Ker Ying Here She\\'s a cripple ! Don\\'t laugh at her Wang Jia Hao Here Wang Jie Hwa Here Wang Wan Wen Here Wang Tian Sheng Here Marble Mister You have a letter from your dad Your Grandma said he\\'s in Panama What has he written ? Just ask your Grandma No My Dad is dead Don\\'t curse him I don\\'t like him Go away Go away Let me go Let me go ... Damn dog Go away Let go of me The damn dog bites anything it sees I always get bitten when delivering letters Don\\'t tell people about it I won\\'t . I\\'ll do as you said You\\'ll be responsible if anyone @ @ @ @ @ @ @ @ @ @ your secret Let\\'s hook our pinkies I\\'ll be true to my word Wait for me ? When I\\'m young no-one likes me I hope someone would love me like crazy If not , I hope I have lots of money If no money , I wish I can be famous Let everyone know me at least If I have nothing else I hope I\\'m prettier than others Staying pretty longer is okay Being a woman is indeed tragic ! But you have everything now How poor of you , silly girl ? You never stay with me overnight You \\'d better find an excuse to convince me My wife doesn\\'t understand me Take any man in the world Not one would say his wife understands him I really love you Not for your money Don\\'t think you\\'re Santa Claus Why would I do this ? So cheap ! Go to bed early Don\\'t abuse drugs Wang , give me a red cloth Okay Lin , give him a red cloth Big one or small one ? Big one , of course Did @ @ @ @ @ @ @ @ @ @ You think old Wang can do that ? You are good Your hand is so white , I believe you \\'d have made it Lin , go to work if you\\'ve nothing else Okay , five dollars Bin , why did you have to come yourself to buy ? What else do you need ? Nothing else . I\\'ll come later See you I know all the neighbours around here Hey , get the button fastened I can\\'t breath if I fasten up Or you buy me a new dress Okay , okay Let me take you to the discount stall Discount stalls only sell cheap stuff I want to visit the fashion shop Look at you Just two months here you\\'ve learned it I learned all about it long before I came Then you be good I\\'ll buy you anything you want Okay ? Then ain\\'t I good enough now ? Good , you\\'re good Sir , I would like to know ... if there\\'s a new villa here ? You\\'ve real talent Among the mistresses of my husband You @ @ @ @ @ @ @ @ @ @ you want ? Sure He\\'s my Santa Claus You think you\\'re young ? Whatever . I\\'m younger than you I heard you\\'re a dancer What kind of dance are you good at ? The kind that takes hard work ... and looks like mad jumping You have money now Why work so hard ? Is money everything ? No ? Depends on the person Don\\'t say you love my man No , we use each other What are you laughing at ? One night your husband told me ... his wife doesn\\'t understand him He lies , you fool It\\'s not bad he lies to me in that way My husband is sure in bad luck He has a mistress like you He is in luck actually Because he met me You\\'re just a whore What are you here for ? You\\'re great Let me show you something No . Okay , you can go now You don\\'t like me a bit ? Like you what ? What good are you ? Oh , I dare not Don\\'t be @ @ @ @ @ @ @ @ @ @ hand Get up , get up Be serious . Come One more time I can\\'t Use the hip . Come . Ready Go See , you can do it Got weak hips . I can\\'t get up Forget it You are a porn star . No need to get up You bald head You should be an extra forever Oh , I am now Yao Ju , come Coming You play with him Hold it Get me a suit from the wardrobe What\\'s that for ? Meet your girlfriend That one You want it however she looks You take all whatever the age My girl is beautiful And who is that ? Nanny ! Yao Ju Who\\'s she ? I don\\'t know Why do you always come to me ? I have things to do Then I\\'ll wait for you You have to wait a long time I\\'ll wait for you Then you have to wait a long time What\\'s wrong with me ? At least I am with single mind Why do you hit her ? Yeah , why do you @ @ @ @ @ @ @ @ @ @ dare I\\'ll kill you You don\\'t want me You dump me after you have had sex with me Isn\\'t it just for money ? No , it isn\\'t for money I love you I am not for money I really love you You\\'re crazy Just make a run Or I\\'ll cut you to pieces Damn shameless bitch Let\\'s play rubber-band rope Come , let\\'s play rubber-band rope Come on Don\\'t go Come In any event let\\'s clarify our relationship today You have a husband My husband is no use Lower your voice What\\'re you afraid of ? No need to fear if you dare to do it In short , I\\'ll give you money Money ... yes , I\\'ll need a lot of it You think you can afford ? You\\'re prepared , huh ? Let me tell you now My cousin will come next month He \\'d need cash for permit and ID card That takes 50,000 dollars Are you worth it ? Maybe I\\'m not worth it But if I spread word of @ @ @ @ @ @ @ @ @ @ And that woman is more trouble enough You think I don\\'t know she\\'s a mistress Just think what if her man knows ... of your affair ! Perhaps you can bear losing face She can\\'t You have no money , she has You hit me ! My husband never dare hit me . How dare you ! Rape ! Help ! I\\'m being raped ... please help ! Help ! Rape ! Someone rapes me Stop it . That will kill her Hey , that\\'s not enough What\\'s that for ? It\\'s for you Carnation Like it ? Carnation is only for mothers Mother\\'s Day ? Do you ever give anything to your Mom ? She wants nothing She only wants money As if someone would marry you ! She\\'ll have three kids in her teens Get fat as a barrel and smell bad She\\'ll get up every day at six-thirty Cook an egg for your breakfast And says it\\'s good for you Every week you go to Kowloon for tea once Every half a year you \\'d take @ @ @ @ @ @ @ @ @ @ ? No At least I have everything But you don\\'t have a husband That\\'s okay to me You say so for sure I\\'ll go now If he doesn\\'t find me on the phone ... then I\\'ll truly have nothing If he lies to his wife , he can lie to you Are you sure he has no-one else ? That\\'s none of your business What about me ? I left that woman because of you What about this ? I don\\'t even know who \\'d get the money I don\\'t care Let go Sit \" RITA \" Miss Rita Last night I passed by your house Your dog scared me Then I fell down And dropped a ring It\\'s my husband\\'s gift You know when a man gives you some gift It\\'s no good if you were to lose it He\\'ll be unhappy Did you see it ? Help me to find it , please The ring is worth 100,000 dollars I\\'ll help you to find it Then I\\'ll wait for your news I\\'m so @ @ @ @ @ @ @ @ @ @ I studied in Grade Two I already hated women That class girl captain ... told the teacher I peeked at her And the teacher hit me hard for it I was hit so hard that it broke the ruler In university I studied the Tang dynasty poems It said women are tender and passionate But I found it to the contrary Women and mean people are hard to deal with It wasn\\'t easy for me to stay away from women Now I wouldn\\'t want one anymore Women are poison ! You\\'re right Let\\'s toast Tonight when you\\'re home You should punish your wife hard All men are brothers ! Great . I\\'ll punish her tonight Fifty to hundred thousand each call Your threat is never-ending After this time ... Once and for all , I\\'ll leave You take this man I\\'m tired of him If you\\'re that crazy for men ... I\\'ll give my Wang to you , too Stupid Where\\'re you going ? Grandma , Grandma . Come on Murder Grandma , hurry What\\'s that serious ? @ @ @ @ @ @ @ @ @ @ ? Over there , you see ? What nonsense ! There\\'s nothing there It\\'s really a murder Hey , stop talking rubbish ! Go to bed Stop looking Kids are innocent Shorty , don\\'t move Stop moving Shorty , Shorty It was terrible last night I saw two guys fight And then one person ... killed the other ! The killer wore a chain You know , I could ... pull out some hair from the killer But Grandma doesn\\'t believe me If you \\'d please check for me What\\'re you doing ? You haven\\'t finished your hair-cut ? What happened ? Go and see Quick What is it ? Quick Come , be quick This is the White Man\\'s house This is the villa This is where Jin Bao lives I live right here That night I saw a murder in there Hey , what\\'s this ? It\\'s a watchband You can\\'t even recognise it in picture , stupid Hold it . Hold it . Hold it What\\'s this ? Silly fool , it\\'s Ninja That night @ @ @ @ @ @ @ @ @ @ saw two guys were fighting over there And then one man ... killed the other The killer wore a watchband too I saw it But I\\'m not scared Then I went out and took a look But he had run away I just saw some hair and a lot of footprints Now our first move ... is to check out who\\'s missing in the village ? What are you doing here ? Where\\'s your wife , uncle ? Looking for my wife at your age ? You have guts Go away I\\'ll get your Dad to take you back You \\'d better spend time on study And stop running around Go home Wang , where\\'s your wife ? My wife is sick , okay ! Nosey Have you seen the White Man and his wife ? Shin , go home for meal Hurry Always playing Marble , time for work Don\\'t run around I\\'m working right now Look here . On the rock Here It\\'s real The White Man Women are sick , vicious ... and fierce . We should kill them @ @ @ @ @ @ @ @ @ @ have to bear with them What is the time ? Oh , time for work Get lost Why are the floor so dirty ? Hey , what\\'re you doing ? I just came early to tidy things up See , what have you done ! I ... White Man . It\\'s him Little girl Little girl Wake up Little girl Wake up You okay ? Are you all right ? But that can\\'t be evidence That\\'s the record in my children\\'s diary This proves that everyday he thought of murder Did your teacher tell you ... not to lie to the police ? I have more evidence Take it out I saw that after he killed his wife ... he dragged the corpse away , too That night he didn\\'t return home He went out for drinks Got drunk and slept on a rock And he cursed the women too He almost killed me What a bad White man Always thinking to kill people Chinese won\\'t do that The Chinese don\\'t have that power Go and see Hi Police ? What\\'re @ @ @ @ @ @ @ @ @ @ illegal to play chess ? You have a warrant , sir ? Of course , see It\\'s marijuana Go away ... What are you looking ? Nothing good to see Move away Hey , what\\'re you doing ? He\\'s my husband He just slapped me slightly Is it illegal the husband slapping his wife ? Actually I let him slap me Why are you arresting him ? Where are you going ? I just went back to my Mama\\'s house Why don\\'t you call me these few days ? Who is he ? His sworn brother What\\'s a \\' sworn brother \\' ? Move . Don\\'t pretend anymore Sir , he\\'s innocent Don\\'t arrest him Girl , don\\'t lie Sir , please don\\'t arrest my husband Don\\'t arrest my husband Let him go Dear Hello , Miss Rita Why ? Why am I being fired ? How do I guess if you don\\'t tell me ? Mister Postman . Mister ? No-one believed the murder I told you about What clue do you have ? I suspected @ @ @ @ @ @ @ @ @ @ if she\\'s dead Wang , you got mail It\\'s hot outside . Come and have a seat Have some tea Busy ? Yeah , the past few days has been killing ! A lot more people at the festival this year ! You ought to take your girlfriend along I don\\'t have a girlfriend You\\'re kidding Where\\'s Mrs . Wang ? Can you not mention her ? Where has she gone ? I don\\'t know where she has gone Mind your own business , kid This letter is from my wife Marble , Marble Oh , you are here ! Shorty has been calling you many times Must have found something new What did she say in the letter ? She ran away with someone Now she\\'s with someone else She told me not to think of her Even though we can\\'t be a couple She still thinks of writing to me I knew she is a good girl Just the men are bad That\\'s good . That bitch is gone Thank God , we\\'ll have peace now Don\\'t @ @ @ @ @ @ @ @ @ @ is murdered What ? I go back to answer the phone Grandma said many girls ... worried that they can\\'t immigrate in 1997 So they all wanted to marry an old foreigner Those old men are so ugly ! More ugly than Wang You see , I don\\'t even have time for toilet Old Wang , just a woman One left . More to come I would get you a good girl later Grandma Everyone will quarrel after getting married Stop being a match-maker If I don\\'t help them make the match ... They will have no-one to quarrel with Let\\'s go home What a crowd ! Look How much is a pack of tissue ? Fifty cents What\\'re you doing ? Idiot I told you Wang\\'s wife is dead No , lousy kid I kept a secret for you What secret ? I won\\'t tell anyone About you and Wang\\'s wife Grandma Xiaoji Xiaoji Xiaoji Xiaoji Marble , get dressed . We go to pray Grandma is bad . You killed my Xiaoji You\\'ll have to compensate No other chicken @ @ @ @ @ @ @ @ @ @ doing ? I\\'m gathering the fire wood Oh , don\\'t sit here Sit over there It\\'ll dirty your pants And you\\'ll need your Grandma to wash it I don\\'t care What did she do ? Did she make you angry ? She\\'s a killer Jin Bao , faster Yes Shorty , Shorty The village chief gave me 10 dollars to catch grasshopper for the bird And you won\\'t investigate my case ? All those you mentioned as missing are still alive I don\\'t know if you\\'re telling the truth That\\'s no fun I didn\\'t lie to you Please help me , okay Unless ... Unless what ? Unless you let me be the commander Well , you listen to me whatever I say The mission we have tonight ... is to search the suspects and evidences Especially that watchband Yes , especially the watchband Jin Bao , you watch number one to fifteen I\\'m terribly afraid Jin Ju , you watch number to thirty Especially my dad , he\\'s fierce How could your Dad be the murderer @ @ @ @ @ @ @ @ @ @ killer , I\\'ll get him You watch all the shops Okay You watch the pigsty and the doghouse You must obey the order You gather information How come I have nothing to do ? Because you walk slow That\\'s true Fine . I\\'m sick of girls crying You take my post Women are troublesome Everyone take a firecracker as weapon , pass it Use it only if you see the enemies The enemies are tough Be careful . It\\'s life-threatening Understand ? Yes Okay , let\\'s go Go If anything please say it now You don\\'t speak and didn\\'t let me either You won\\'t understand even if I say it Has your husband been found ? In this situation Spreading out isn\\'t the worst . At the most we could flee How \\'d that help ? I am not afraid of getting caught I know I am poor All you love is money Help ! Help ! Shorty , don\\'t be afraid Don\\'t be afraid Come back Shorty\\'s spirit , come back Don\\'t be afraid Shorty\\'s @ @ @ @ @ @ @ @ @ @ ? Who is it ? Ghost ! Open up your eye Shorty , recover Let\\'s stop the investigation , okay ? We just want you to recover Yes Yes , don\\'t be like that Shorty , don\\'t be afraid ... please ! Shorty\\'s spirit , come back Shorty\\'s spirit , come back What took so long ? Is it okay ? It\\'s not done yet Mine is fit for eating ? Marble , come to eat Shorty\\'s gone mad . I have no appetite Hey , come over I\\'ll let you be the host . Come It smells great Fire\\'s up It\\'s ready Don\\'t kill her Are you alright ? Grandma will never kill your chicken I\\'ll buy you another chicken tomorrow I swear I\\'ll never kill your chick You hear me ? What\\'re you doing ? It\\'s all your fault I could watch you once I can\\'t watch you forever I\\'m not willing to do that I didn\\'t hate you We had been a couple before I\\'ll burn more food @ @ @ @ @ @ @ @ @ @\\'ll offer some Chau Yun Fa\\'s photos , too Wang ... I need to talk to you urgently Open the door Wang , I have something to tell you You \\'d better brace yourself for this Don\\'t be shocked out of mind Actually your wife is dead I knew you would be stunned Relax and listen to me The letters you got It\\'s all a farce It\\'s actually the postman The postman did it After he killed your wife ... He forged her letter to you He knew how The school taught us ... how to write to ourselves Let me see the letter . Quick Then I\\'ll get it for myself So , will you leave ? Where do we go ? To Mainland China , okay ? Whatever You don\\'t leave until you got trouble Now you got trouble Will this all ever be exposed ? Did you leave any evidence ? The trouble is the letter you wrote What letter ? The letter you forged in her handwriting I had never written any letter So the letter Wang had received ... @ @ @ @ @ @ @ @ @ @ Marble Come . Eat something This is yours Go on ! How come I have more ? And you have less ? You take this bowl It\\'s okay for kids to eat more It\\'s sweet and tasty . Eat now Where\\'s the letter ? Oh , I forgot where I put them Eat ... It\\'s new Yes . Eat while it\\'s hot Relax , Wang Everything will come out fine He can\\'t run away I\\'ll move on Or grandma will scold me , bye Bye Brother Yao Where were you last night ? It\\'s late . Time to go Are you ready ? Yes You\\'re always the last Let\\'s go Where\\'s Marble ? Marble is up the hill with Wang Ready , go You see , it flies high It\\'s great It\\'s beautiful Marble , you fly it It\\'s fun Be careful . Be careful What happened ? Marble , Marble What are you doing here ? There\\'s a kite there Yes , go have a look You see Jin Bao , be careful Be @ @ @ @ @ @ @ @ @ @ down It hurts Murderer It\\'s not us He fell down by himself . It\\'s not us Where\\'s Marble ? She was just here Help ... ! Help ! Help ... ! Help ! Mister Postman ... Don\\'t be afraid Help me Mister Postman Help me Mister Postman Marble , grab my hand Wang Inspector , see Let me Mr . Butterfly give you a lesson Do you know how to act ? Your first day at this , huh ? Sorry , Brother Lang Sorry Hey , what\\'s up ? What\\'s the matter ? They are all fools Yes , he\\'s a fool Be smart Once again Let\\'s do it again One try is okay Calm down Come again Do it better All standby Ready You\\'re shooting movies ? Master , who do you fight with ? That\\'s Wan Ji Lang ? Want a drink , master ? Who\\'s the director ? The one in red glasses A woman Yes Oh no , fell down ! Brother Ji Lang , you okay ? What\\'s wrong with you ? @ @ @ @ @ @ @ @ @ @ , come over Coming It\\'s my turn . Don\\'t run around It\\'s all yours Yao Ju . Memorize those moves Act better Ready All standby Do it better Don\\'t make trouble Ready ? Action ! Cut Okay Move the machine here Move it over Come , over here Like that ? Okay ? It\\'s fine You said you \\'d go to Mainland with me Really ? Of course you can\\'t remember I did Did you see me shooting movie ? A little bit Next week I\\'ll go to London I will write to you when I arrive You better not go If you don\\'t go , you don\\'t need to write You \\'d pay for my living , huh ? No big deal I don\\'t know what to say That\\'s rubbish Okay Take care Mister Postman Marble , are you alright ? Yes Marble ... It\\'s for you Is it good ? Do you like it ? \\r\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_11[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method cleaned a few basic formatting errors. You are welcome to experiment and add your own code to expand on cleaning the text. Once it begins to look the way you want it to, we can start using spaCy to do some heavier cleaning. spaCy does this through the use of language models. Make sure you have the spaCy english language model loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is going to be the basis of most of the tasks which NLTK (the Natural Language Tool Kit) might have done otherwise. While NLTK uses a bunch of clunky models from multiple places, spacy neatly packs all functionality in one model. This deep trained model will start by doing our word tokenizing and normalising. These methods are already included in lucem illud, and here we will just demonstrate them outside the package so to explain what goes on under the hood.\n",
    "\n",
    "When we run text through a spacy model, it performs a lot of computation on each of the tokens. Each token is given certain attributes, such as a part of speech, information whether it is a number, a stop word, and so on. \n",
    "\n",
    "To perform a tokenization, we check if the string includes punctuation and add this token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(word_list):\n",
    "    tokenized = []\n",
    "    # pass word list through language model.\n",
    "    doc = nlp(word_list)\n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            tokenized.append(token.text)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@@216680',\n",
       " 'Hey',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'talking',\n",
       " 'to',\n",
       " 'you',\n",
       " 'Give',\n",
       " 'me',\n",
       " '600',\n",
       " 'dollars',\n",
       " 'You',\n",
       " 'wish',\n",
       " 'That',\n",
       " \"'s\",\n",
       " 'all',\n",
       " 'we',\n",
       " \"'ve\",\n",
       " 'left',\n",
       " 'And',\n",
       " 'you',\n",
       " 'still',\n",
       " 'go',\n",
       " 'to',\n",
       " 'gamble',\n",
       " 'Shut',\n",
       " 'up',\n",
       " 'I',\n",
       " 'earn',\n",
       " 'the',\n",
       " 'money',\n",
       " 'Even',\n",
       " 'that',\n",
       " 'you',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'take',\n",
       " 'it',\n",
       " 'for',\n",
       " 'gamble',\n",
       " 'Shut',\n",
       " 'up',\n",
       " 'What',\n",
       " \"'re\",\n",
       " 'you',\n",
       " 'doing',\n",
       " 'Bastard',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'gon',\n",
       " 'na',\n",
       " 'beat',\n",
       " 'you',\n",
       " 'You',\n",
       " 'gambling',\n",
       " 'pig',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'beat',\n",
       " 'the',\n",
       " 'shit',\n",
       " 'out',\n",
       " 'of',\n",
       " 'you',\n",
       " 'You',\n",
       " 'bitch',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'beat',\n",
       " 'you',\n",
       " 'You',\n",
       " 'dare',\n",
       " 'to',\n",
       " 'hit',\n",
       " 'me',\n",
       " 'with',\n",
       " 'something',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'kill',\n",
       " 'you',\n",
       " 'All',\n",
       " 'you',\n",
       " 'know',\n",
       " 'is',\n",
       " 'gambling',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'beat',\n",
       " 'you',\n",
       " 'What',\n",
       " \"'re\",\n",
       " 'you',\n",
       " 'doing',\n",
       " 'Let',\n",
       " 'go',\n",
       " 'of',\n",
       " 'me',\n",
       " 'Stop',\n",
       " 'You',\n",
       " \"'ll\",\n",
       " 'kill',\n",
       " 'Mom',\n",
       " 'Mom',\n",
       " 'are',\n",
       " 'you',\n",
       " 'all',\n",
       " 'right',\n",
       " 'Do',\n",
       " \"n't\",\n",
       " 'touch',\n",
       " 'my',\n",
       " 'money',\n",
       " 'Dad',\n",
       " 'where',\n",
       " \"'re\",\n",
       " 'you',\n",
       " 'going',\n",
       " 'Go',\n",
       " 'away',\n",
       " 'Dad',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'go',\n",
       " 'Get',\n",
       " 'lost',\n",
       " 'Dad',\n",
       " 'come',\n",
       " 'back',\n",
       " 'You',\n",
       " 'take',\n",
       " 'Marble',\n",
       " 'with',\n",
       " 'you',\n",
       " 'The',\n",
       " 'way',\n",
       " 'she',\n",
       " \"'s\",\n",
       " 'now',\n",
       " 'Do',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'give',\n",
       " 'me',\n",
       " 'trouble',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'in',\n",
       " 'more',\n",
       " 'trouble',\n",
       " 'than',\n",
       " 'you',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'a',\n",
       " 'sailor',\n",
       " 'You',\n",
       " 'take',\n",
       " 'care',\n",
       " 'of',\n",
       " 'Marble',\n",
       " 'Just',\n",
       " 'gamble',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'less',\n",
       " 'is',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'raise',\n",
       " 'the',\n",
       " 'kid',\n",
       " 'No',\n",
       " 'way',\n",
       " 'Raising',\n",
       " 'her',\n",
       " 'would',\n",
       " 'be',\n",
       " 'like',\n",
       " 'raising',\n",
       " 'others',\n",
       " 'take',\n",
       " 'care',\n",
       " 'of',\n",
       " 'her',\n",
       " 'Marry',\n",
       " 'her',\n",
       " 'when',\n",
       " 'she',\n",
       " 'grows',\n",
       " 'up',\n",
       " 'And',\n",
       " 'you',\n",
       " 'tell',\n",
       " 'her',\n",
       " 'to',\n",
       " 'be',\n",
       " 'smart',\n",
       " 'Do',\n",
       " \"n't\",\n",
       " 'marry',\n",
       " 'a',\n",
       " 'gambler',\n",
       " 'Hey',\n",
       " 'you',\n",
       " 'want',\n",
       " 'a',\n",
       " 'quarrel',\n",
       " 'No',\n",
       " 'never',\n",
       " 'This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'last',\n",
       " 'time',\n",
       " 'You',\n",
       " \"'re\",\n",
       " 'SO',\n",
       " 'right',\n",
       " 'Do',\n",
       " \"n't\",\n",
       " 'rush',\n",
       " 'Everyone',\n",
       " 'will',\n",
       " 'have',\n",
       " 'a',\n",
       " 'share',\n",
       " 'Why',\n",
       " 'so',\n",
       " 'crowded',\n",
       " 'What',\n",
       " \"'re\",\n",
       " 'they',\n",
       " 'doing',\n",
       " 'Worshipping',\n",
       " 'ancestors',\n",
       " 'With',\n",
       " 'all',\n",
       " 'those',\n",
       " 'many',\n",
       " 'things',\n",
       " 'Hey',\n",
       " 'Granny',\n",
       " 'Come',\n",
       " 'on',\n",
       " 'Come',\n",
       " 'and',\n",
       " 'take',\n",
       " 'a',\n",
       " 'bite',\n",
       " 'The',\n",
       " 'dishes',\n",
       " 'are',\n",
       " 'good',\n",
       " 'this',\n",
       " 'year',\n",
       " 'Right',\n",
       " 'Marble',\n",
       " 'have',\n",
       " 'some',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'go',\n",
       " 'get',\n",
       " 'my',\n",
       " 'share',\n",
       " 'of',\n",
       " 'pork',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'leg',\n",
       " 'washing',\n",
       " 'basin',\n",
       " 'Hey',\n",
       " 'how',\n",
       " 'can',\n",
       " 'you',\n",
       " 'say',\n",
       " 'that',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'go',\n",
       " 'get',\n",
       " 'a',\n",
       " 'share',\n",
       " 'of',\n",
       " 'pork',\n",
       " 'Your',\n",
       " 'share',\n",
       " 'You',\n",
       " 'wish',\n",
       " 'That',\n",
       " \"'s\",\n",
       " 'only',\n",
       " 'for',\n",
       " 'boys',\n",
       " 'Yeah',\n",
       " 'I',\n",
       " 'have',\n",
       " 'a',\n",
       " 'wee',\n",
       " 'wee',\n",
       " 'Have',\n",
       " 'you',\n",
       " 'Aunty',\n",
       " 'Chiang',\n",
       " 'Lin',\n",
       " 'one',\n",
       " 'more',\n",
       " 'baby',\n",
       " 'this',\n",
       " 'year',\n",
       " 'Yes',\n",
       " 'You',\n",
       " 'are',\n",
       " 'the',\n",
       " 'best',\n",
       " 'in',\n",
       " 'your',\n",
       " 'family',\n",
       " 'Hey',\n",
       " 'you',\n",
       " 'think',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'my',\n",
       " 'will',\n",
       " 'I',\n",
       " 'was',\n",
       " 'compelled',\n",
       " 'by',\n",
       " 'my',\n",
       " 'stupid',\n",
       " 'husband',\n",
       " 'Come',\n",
       " 'on',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'worship',\n",
       " 'first',\n",
       " 'Eat',\n",
       " 'some',\n",
       " 'roast',\n",
       " 'pork',\n",
       " 'We',\n",
       " 'have',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'it',\n",
       " 'Honey',\n",
       " 'baby',\n",
       " 'Yes',\n",
       " 'Only',\n",
       " 'with',\n",
       " 'Chinese',\n",
       " 'blood',\n",
       " 'What',\n",
       " 'do',\n",
       " 'you',\n",
       " 'mean',\n",
       " 'It',\n",
       " 'means',\n",
       " 'I',\n",
       " 'give',\n",
       " 'you',\n",
       " 'a',\n",
       " 'green',\n",
       " 'hat',\n",
       " 'Oh',\n",
       " 'that',\n",
       " 'goes',\n",
       " 'with',\n",
       " 'my',\n",
       " 'clothes',\n",
       " 'then',\n",
       " 'Eat',\n",
       " 'no',\n",
       " 'more',\n",
       " 'talk',\n",
       " 'So',\n",
       " 'rude',\n",
       " 'Take',\n",
       " 'it',\n",
       " 'home',\n",
       " 'Get',\n",
       " 'up',\n",
       " 'go',\n",
       " 'We',\n",
       " \"'re\",\n",
       " 'going',\n",
       " 'back',\n",
       " 'home',\n",
       " 'Be',\n",
       " 'quick',\n",
       " 'Honey',\n",
       " 'Hey',\n",
       " 'make',\n",
       " 'a',\n",
       " 'stance',\n",
       " 'Hold',\n",
       " 'it',\n",
       " 'Master',\n",
       " 'every',\n",
       " 'time',\n",
       " 'we',\n",
       " \"'ve\",\n",
       " 'to',\n",
       " 'stand',\n",
       " 'in',\n",
       " 'a',\n",
       " 'pose',\n",
       " 'If',\n",
       " 'you',\n",
       " 'could',\n",
       " 'teach',\n",
       " 'us',\n",
       " 'something',\n",
       " 'else',\n",
       " 'please',\n",
       " 'Okay',\n",
       " 'Shaolin',\n",
       " 'Masters',\n",
       " 'learn',\n",
       " 'poses',\n",
       " 'for',\n",
       " 'years',\n",
       " 'Hold',\n",
       " 'it',\n",
       " 'Okay',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'teach',\n",
       " 'you',\n",
       " 'something',\n",
       " 'new',\n",
       " 'Watch',\n",
       " 'me',\n",
       " 'See',\n",
       " 'Get',\n",
       " 'to',\n",
       " 'it',\n",
       " 'Seeing',\n",
       " 'Shanghai',\n",
       " 'Beach',\n",
       " 'in',\n",
       " 'Mainland',\n",
       " 'Here',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'still',\n",
       " 'watching',\n",
       " 'it',\n",
       " 'everyday',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'so',\n",
       " 'tired',\n",
       " 'Tired',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'go',\n",
       " 'to',\n",
       " 'bed',\n",
       " 'for',\n",
       " 'rest',\n",
       " 'I',\n",
       " 'hate',\n",
       " 'Chau',\n",
       " 'Yun',\n",
       " 'Fa',\n",
       " 'the',\n",
       " 'most',\n",
       " 'He',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'marry',\n",
       " 'when',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'in',\n",
       " 'China',\n",
       " 'As',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'here',\n",
       " 'he',\n",
       " 'got',\n",
       " 'married',\n",
       " 'You',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'come',\n",
       " 'to',\n",
       " 'Hong',\n",
       " 'Kong',\n",
       " 'if',\n",
       " 'you',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'marry',\n",
       " 'me',\n",
       " 'You',\n",
       " 'said',\n",
       " 'you',\n",
       " 'are',\n",
       " 'big',\n",
       " 'boss',\n",
       " 'Having',\n",
       " 'a',\n",
       " 'big',\n",
       " 'business',\n",
       " 'Now',\n",
       " 'that',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'in',\n",
       " 'Hong',\n",
       " 'Kong',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'still',\n",
       " 'for',\n",
       " 'myself',\n",
       " 'Then',\n",
       " 'I',\n",
       " 'ought',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'much',\n",
       " 'more',\n",
       " 'sorry',\n",
       " 'You',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'even',\n",
       " 'lay',\n",
       " 'an',\n",
       " 'egg',\n",
       " 'This',\n",
       " 'year',\n",
       " 'I',\n",
       " 'can',\n",
       " 'only',\n",
       " 'get',\n",
       " 'one',\n",
       " 'share',\n",
       " 'of',\n",
       " 'pork',\n",
       " 'And',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'blaming',\n",
       " 'me',\n",
       " 'for',\n",
       " 'it',\n",
       " 'Damn',\n",
       " 'monkey',\n",
       " 'What',\n",
       " \"'s\",\n",
       " 'going',\n",
       " 'on',\n",
       " 'A',\n",
       " 'letter',\n",
       " 'for',\n",
       " 'you',\n",
       " 'Oh',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'you',\n",
       " 'Your',\n",
       " 'monkey',\n",
       " 'glared',\n",
       " 'at',\n",
       " 'me',\n",
       " 'fiercely',\n",
       " 'Will',\n",
       " 'you',\n",
       " 'open',\n",
       " 'it',\n",
       " 'for',\n",
       " 'me',\n",
       " 'Read',\n",
       " 'it',\n",
       " 'to',\n",
       " 'me',\n",
       " 'I',\n",
       " 'am',\n",
       " 'quite',\n",
       " 'illiterate',\n",
       " 'Stop',\n",
       " 'it',\n",
       " 'Do',\n",
       " \"n't\",\n",
       " 'bother',\n",
       " 'him',\n",
       " 'Let',\n",
       " 'me',\n",
       " 'help',\n",
       " 'you',\n",
       " 'Give',\n",
       " 'me',\n",
       " 'the',\n",
       " 'letter',\n",
       " 'Postman',\n",
       " \"'s\",\n",
       " 'duty',\n",
       " 'is',\n",
       " 'to',\n",
       " 'deliver',\n",
       " 'and',\n",
       " 'read',\n",
       " 'letters',\n",
       " 'He',\n",
       " 'reads',\n",
       " 'for',\n",
       " 'Aunt',\n",
       " 'Jiao',\n",
       " 'How',\n",
       " 'can',\n",
       " 'you',\n",
       " 'compare',\n",
       " 'to',\n",
       " 'her',\n",
       " 'Aunty',\n",
       " 'Jiao',\n",
       " 'is',\n",
       " '83',\n",
       " 'years',\n",
       " 'old',\n",
       " 'Hello',\n",
       " 'dubbing',\n",
       " 'room',\n",
       " 'Martin',\n",
       " 'Martin',\n",
       " 'is',\n",
       " 'working',\n",
       " 'Are',\n",
       " 'you',\n",
       " 'lying',\n",
       " 'to',\n",
       " 'me',\n",
       " 'Why',\n",
       " 'should',\n",
       " 'I',\n",
       " 'lie',\n",
       " 'I',\n",
       " 'know',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'free',\n",
       " 'Get',\n",
       " 'him',\n",
       " 'You',\n",
       " 'listen',\n",
       " 'yourself',\n",
       " 'then',\n",
       " 'Hello',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'noise',\n",
       " 'Is',\n",
       " 'someone',\n",
       " 'being',\n",
       " 'raped',\n",
       " 'Martin',\n",
       " 'your',\n",
       " 'telephone',\n",
       " 'Hello',\n",
       " 'What',\n",
       " 'took',\n",
       " 'you',\n",
       " 'so',\n",
       " 'long',\n",
       " 'Recording',\n",
       " 'is',\n",
       " 'going',\n",
       " 'on',\n",
       " 'Do',\n",
       " \"n't\",\n",
       " 'mess',\n",
       " 'around',\n",
       " 'Yes',\n",
       " 'I',\n",
       " 'know',\n",
       " 'I',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'Honey',\n",
       " 'did',\n",
       " 'you',\n",
       " 'spend',\n",
       " 'so',\n",
       " 'much',\n",
       " 'I',\n",
       " 'was',\n",
       " 'late',\n",
       " 'for',\n",
       " 'work',\n",
       " 'so',\n",
       " 'I',\n",
       " 'took',\n",
       " 'a',\n",
       " 'taxi',\n",
       " 'On',\n",
       " 'your',\n",
       " 'way',\n",
       " 'back',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'forget',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'vegetables',\n",
       " 'Okay',\n",
       " 'And',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'pair',\n",
       " 'of',\n",
       " 'slippers',\n",
       " 'for',\n",
       " 'me',\n",
       " 'Got',\n",
       " 'it',\n",
       " 'honey',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'hang',\n",
       " 'up',\n",
       " 'Bye',\n",
       " 'Would',\n",
       " \"n't\",\n",
       " 'marry',\n",
       " 'if',\n",
       " 'you',\n",
       " 'know',\n",
       " 'your',\n",
       " 'wife',\n",
       " 'is',\n",
       " 'so',\n",
       " 'bad',\n",
       " 'Take',\n",
       " 'pity',\n",
       " 'for',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'my',\n",
       " 'life',\n",
       " 'Hey',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'on',\n",
       " 'your',\n",
       " 'neck',\n",
       " 'Nothing',\n",
       " 'You',\n",
       " 'look',\n",
       " 'piteous',\n",
       " 'She',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'piteous',\n",
       " 'one',\n",
       " 'The',\n",
       " 'vampire',\n",
       " 'is',\n",
       " 'here',\n",
       " 'Grandma',\n",
       " 'grandma',\n",
       " 'Someone',\n",
       " 'is',\n",
       " 'fighting',\n",
       " 'Grandma',\n",
       " 'grandma',\n",
       " 'Come',\n",
       " 'over',\n",
       " 'quickly',\n",
       " 'What',\n",
       " 'happened',\n",
       " 'Someone',\n",
       " 'is',\n",
       " 'fighting',\n",
       " 'Fighting',\n",
       " 'Yes',\n",
       " 'Where',\n",
       " 'Over',\n",
       " 'there',\n",
       " 'You',\n",
       " 'see',\n",
       " 'it',\n",
       " 'I',\n",
       " 'spare',\n",
       " 'you',\n",
       " 'this',\n",
       " 'time',\n",
       " 'No',\n",
       " 'next',\n",
       " 'time',\n",
       " 'Sorry',\n",
       " 'let',\n",
       " 'me',\n",
       " 'kiss',\n",
       " 'you',\n",
       " 'Cut',\n",
       " 'the',\n",
       " 'bullshit',\n",
       " 'Go',\n",
       " 'to',\n",
       " 'bed',\n",
       " 'I',\n",
       " 'need',\n",
       " 'to',\n",
       " 'work',\n",
       " 'tomorrow',\n",
       " 'Where',\n",
       " 'at',\n",
       " 'That',\n",
       " 'villa',\n",
       " 'over',\n",
       " 'there',\n",
       " 'Wow',\n",
       " 'it',\n",
       " 'looks',\n",
       " 'beautiful',\n",
       " 'Who',\n",
       " 'lives',\n",
       " 'there',\n",
       " 'Snow',\n",
       " 'White',\n",
       " 'Kidding',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'no',\n",
       " 'Snow',\n",
       " 'White',\n",
       " 'Oh',\n",
       " 'murder',\n",
       " 'That',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'pig',\n",
       " 'scream',\n",
       " 'Why',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'pig',\n",
       " 'screaming',\n",
       " 'so',\n",
       " 'late',\n",
       " 'at',\n",
       " 'night',\n",
       " 'Shorty',\n",
       " \"'s\",\n",
       " 'late',\n",
       " 'So',\n",
       " 'the',\n",
       " 'government',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'But',\n",
       " 'how',\n",
       " 'could',\n",
       " 'a',\n",
       " 'pig',\n",
       " \"'s\",\n",
       " 'scream',\n",
       " 'be',\n",
       " 'so',\n",
       " 'terrible',\n",
       " 'Why',\n",
       " 'Why',\n",
       " 'are',\n",
       " \"n't\",\n",
       " 'you',\n",
       " 'tired',\n",
       " 'Why',\n",
       " 'are',\n",
       " \"n't\",\n",
       " 'you',\n",
       " 'sleepy',\n",
       " 'Why',\n",
       " 'have',\n",
       " \"n't\",\n",
       " 'you',\n",
       " 'gone',\n",
       " 'to',\n",
       " 'bed',\n",
       " 'Why',\n",
       " 'are',\n",
       " 'you',\n",
       " 'so',\n",
       " 'nosy',\n",
       " 'Go',\n",
       " 'to',\n",
       " 'sleep',\n",
       " 'Here',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'kitchen',\n",
       " 'Oil',\n",
       " 'Salt',\n",
       " 'Sauce',\n",
       " 'Vinegar',\n",
       " 'All',\n",
       " 'there',\n",
       " 'Go',\n",
       " 'tidy',\n",
       " 'up',\n",
       " 'Ask',\n",
       " 'me',\n",
       " 'if',\n",
       " 'you',\n",
       " 'have',\n",
       " 'any',\n",
       " 'question',\n",
       " 'Come',\n",
       " 'dance',\n",
       " 'with',\n",
       " 'me',\n",
       " 'Come',\n",
       " 'on',\n",
       " 'Come',\n",
       " 'Dance',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'teach',\n",
       " 'you',\n",
       " 'Do',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'afraid',\n",
       " 'Damn',\n",
       " 'dog',\n",
       " 'Damn',\n",
       " 'it',\n",
       " 'What',\n",
       " \"'s\",\n",
       " 'such',\n",
       " 'a',\n",
       " 'big',\n",
       " 'deal',\n",
       " 'to',\n",
       " 'be',\n",
       " 'rich',\n",
       " 'Pay',\n",
       " 'for',\n",
       " 'my',\n",
       " 'pants',\n",
       " 'Sorry',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'mean',\n",
       " 'to',\n",
       " 'be',\n",
       " 'rude',\n",
       " 'Be',\n",
       " 'careful',\n",
       " 'next',\n",
       " 'time',\n",
       " 'Miss',\n",
       " 'it',\n",
       " 'seems',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'seen',\n",
       " 'you',\n",
       " 'somewhere',\n",
       " 'Of',\n",
       " 'course',\n",
       " 'you',\n",
       " 'have',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'be',\n",
       " 'the',\n",
       " 'Miss',\n",
       " 'Hong',\n",
       " 'Kong',\n",
       " 'in',\n",
       " 'next',\n",
       " 'contest',\n",
       " 'I',\n",
       " 'have',\n",
       " 'nine',\n",
       " 'posters',\n",
       " 'at',\n",
       " 'the',\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(clean_11[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the sentence is broken into its constituent words. We can then do some analysis with this. \n",
    "\n",
    "Note: we still don't know which movie this is, just that it is raw movie text data. Maybe in the process of our text cleaning and counting we will come across something!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words\n",
    "\n",
    "If we want to do some analysis we can start by simply counting the number of times each word occurs within the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCounter(wordLst):\n",
    "    wordCounts = {}\n",
    "    for word in wordLst:\n",
    "        #We usually need to normalize the case\n",
    "        wLower = word.lower()\n",
    "        if wLower in wordCounts:\n",
    "            wordCounts[wLower] += 1\n",
    "        else:\n",
    "            wordCounts[wLower] = 1\n",
    "    #convert to DataFrame\n",
    "    countsForFrame = {'word' : [], 'count' : []}\n",
    "    for w, c in wordCounts.items():\n",
    "        countsForFrame['word'].append(w)\n",
    "        countsForFrame['count'].append(c)\n",
    "    return pandas.DataFrame(countsForFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@@216680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'m</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>talking</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>you</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>give</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>me</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  count\n",
       "0  @@216680      1\n",
       "1       hey     16\n",
       "2         i    191\n",
       "3        'm     24\n",
       "4   talking      2\n",
       "5        to    112\n",
       "6       you    302\n",
       "7      give     14\n",
       "8        me     82\n",
       "9       600      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countedWords = wordCounter(word_tokenize(clean_11[1]))\n",
    "countedWords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets sort and plot our counts to investigate the shape of our word frequency distribution.\n",
    "\n",
    "First we need to sort the words by count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>you</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>'s</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>it</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>postman</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>please</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>but</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>as</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>say</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  count\n",
       "6        you    302\n",
       "2          i    191\n",
       "13        's    126\n",
       "5         to    112\n",
       "31        it    109\n",
       "..       ...    ...\n",
       "248  postman     10\n",
       "187   please      9\n",
       "328      but      9\n",
       "215       as      9\n",
       "135      say      9\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Doing this in place (changing the original DataFrame) as we don't need the unsorted DataFrame\n",
    "countedWords.sort_values('count', ascending=False, inplace=True)\n",
    "countedWords[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation and very common words (articles 'a' and 'the'; prepositions 'of' and 'to') make up the most common values, but this isn't very interesting and can actually get in the way of our analysis. We may remove these 'function words' by removing according to a stopword list, setting some frequency threshold, or using a weighting scheme (like tf.idf) to decrease their influence - we will look at these methods in more detail as we go through this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creating a figure and axis lets us do things like change the scaling or add a title\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(range(len(countedWords)), countedWords['count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that likelihood of a word occurring is inversely proportional to its rank. This effect is called [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law), and suggests that the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc. Zipf's law is most easily observed by plotting the data on a log-log graph, with the axes being log (rank order) and log (frequency) resulting in a linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(range(len(countedWords)), countedWords['count'])\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The same relationship occurs in many other rankings, such as population ranks of cities, corporation sizes, income rankings, etc.) \n",
    "\n",
    "The distribution was imagined by Zipf to be driven by a principle of 'least effort' where speakers did not work any harder than necessary to communicate a given idea, but the basis for this relationship is still not well understood and conforms at least as well to a process of [preferential attachment](https://en.wikipedia.org/wiki/Preferential_attachment) whereby people disproportionately attend to popular words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and working with raw text\n",
    "\n",
    "First, we will retrieve a collection of press releases from a *GitHub API* that archived them, based on a number of analyses by Justin Grimmer, a political scientist whose work we will read in two weeks!\n",
    "\n",
    "GitHub API requests are made to `'https://api.github.com/'` and responses are in JSON, similar to Tumblr's API.\n",
    "\n",
    "We will get the information on [github.com/lintool/GrimmerSenatePressReleases](https://github.com/lintool/GrimmerSenatePressReleases) as it contains a nice set documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'node_id', 'name', 'full_name', 'private', 'owner', 'html_url', 'description', 'fork', 'url', 'forks_url', 'keys_url', 'collaborators_url', 'teams_url', 'hooks_url', 'issue_events_url', 'events_url', 'assignees_url', 'branches_url', 'tags_url', 'blobs_url', 'git_tags_url', 'git_refs_url', 'trees_url', 'statuses_url', 'languages_url', 'stargazers_url', 'contributors_url', 'subscribers_url', 'subscription_url', 'commits_url', 'git_commits_url', 'comments_url', 'issue_comment_url', 'contents_url', 'compare_url', 'merges_url', 'archive_url', 'downloads_url', 'issues_url', 'pulls_url', 'milestones_url', 'notifications_url', 'labels_url', 'releases_url', 'deployments_url', 'created_at', 'updated_at', 'pushed_at', 'git_url', 'ssh_url', 'clone_url', 'svn_url', 'homepage', 'size', 'stargazers_count', 'watchers_count', 'language', 'has_issues', 'has_projects', 'has_downloads', 'has_wiki', 'has_pages', 'forks_count', 'mirror_url', 'archived', 'disabled', 'open_issues_count', 'license', 'forks', 'open_issues', 'watchers', 'default_branch', 'temp_clone_token', 'network_count', 'subscribers_count'])\n",
      "Grimmer's Senate Press Releases\n"
     ]
    }
   ],
   "source": [
    "r = requests.get('https://api.github.com/repos/lintool/GrimmerSenatePressReleases')\n",
    "senateReleasesData = json.loads(r.text)\n",
    "print(senateReleasesData.keys())\n",
    "print(senateReleasesData['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are interested in here is the `'contents_url'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/{+path}\n"
     ]
    }
   ],
   "source": [
    "print(senateReleasesData['contents_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to get any or all of the files from the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '10Apr2007Whitehouse123.txt',\n",
       " 'path': 'raw/Whitehouse/10Apr2007Whitehouse123.txt',\n",
       " 'sha': 'f524289ee563dca58690c8d36c23dce5dbd9962a',\n",
       " 'size': 2206,\n",
       " 'url': 'https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/raw/Whitehouse/10Apr2007Whitehouse123.txt?ref=master',\n",
       " 'html_url': 'https://github.com/lintool/GrimmerSenatePressReleases/blob/master/raw/Whitehouse/10Apr2007Whitehouse123.txt',\n",
       " 'git_url': 'https://api.github.com/repos/lintool/GrimmerSenatePressReleases/git/blobs/f524289ee563dca58690c8d36c23dce5dbd9962a',\n",
       " 'download_url': 'https://raw.githubusercontent.com/lintool/GrimmerSenatePressReleases/master/raw/Whitehouse/10Apr2007Whitehouse123.txt',\n",
       " 'type': 'file',\n",
       " '_links': {'self': 'https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/raw/Whitehouse/10Apr2007Whitehouse123.txt?ref=master',\n",
       "  'git': 'https://api.github.com/repos/lintool/GrimmerSenatePressReleases/git/blobs/f524289ee563dca58690c8d36c23dce5dbd9962a',\n",
       "  'html': 'https://github.com/lintool/GrimmerSenatePressReleases/blob/master/raw/Whitehouse/10Apr2007Whitehouse123.txt'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get('https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/raw/Whitehouse')\n",
    "whitehouseLinks = json.loads(r.text)\n",
    "whitehouseLinks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of information about Whitehouse press releases. Let's look at one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEN. WHITEHOUSE SHARES WESTERLY GIRL'S STORY IN PUSH FOR STEM CELL RESEARCH\n",
      "  Sharing the story of Lila Barber, a 12 year old girl from Westerly, Sen. Sheldon Whitehouse (D-R.I.) on Tuesday, April 10, 2007, illustrated the hope stem cell research can offer in a speech on the Senate floor in favor of legislation to expand federal funding for stem cell research.  \n",
      "   Whitehouse met Lila two weeks ago. She was diagnosed two years ago with osteosarcoma, a cancerous bone condition, and last year underwent cadaver bone transplant surgery. The procedure saved her leg and is helping her remain cancer-free, but the transplanted tissue will not grow with her and likely will break down over time. Stem cell research, Whitehouse explained, could vastly improve the care of patients like Lila by allowing surgeons to enhance transplants with a patient's own stem cells, which could replace the lost bone and cartilage, or grow entirely new replacement bones and joints. \n",
      "   \"Stem cell research gives hope\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2206"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(whitehouseLinks[0]['download_url'])\n",
    "whitehouseRelease = r.text\n",
    "print(whitehouseRelease[:1000])\n",
    "len(whitehouseRelease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a string of correctly encoded text. For analysis of its words, next we need to tokenize it, or split it into a sequence of tokens or word instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STEM',\n",
       " 'CELL',\n",
       " 'RESEARCH',\n",
       " 'Sharing',\n",
       " 'the',\n",
       " 'story',\n",
       " 'of',\n",
       " 'Lila',\n",
       " 'Barber',\n",
       " 'a',\n",
       " '12',\n",
       " 'year',\n",
       " 'old',\n",
       " 'girl',\n",
       " 'from',\n",
       " 'Westerly',\n",
       " 'Sen.',\n",
       " 'Sheldon',\n",
       " 'Whitehouse',\n",
       " 'D']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whTokens = word_tokenize(whitehouseRelease)\n",
    "whTokens[10:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous iterations of the course used `nltk`s tokenizer, but as we saw before, we used spaCy's model (https://spacy.io/api/tokenizer). We intend to avoid using NLTK when we can - for production level code, it is a lot faster and cleaner to use spaCy.\n",
    "\n",
    "That being said, `nltk` does have some useful methods for quick analysis of small corpora. We will explore a few here, and encourage you to figure out for yourself which other ones might be useful.\n",
    "\n",
    "To use the list of tokens in `nltk`, and take advantage of functions like `concordance`, shown above, we can convert it into a `Text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "ld girl from Westerly Sen. Sheldon Whitehouse D R.I. on Tuesday April 10 2007 il\n",
      "ral funding for stem cell research Whitehouse met Lila two weeks ago She was dia\n",
      " down over time Stem cell research Whitehouse explained could vastly improve the\n",
      "s research might help or even cure Whitehouse said Whitehouse also praised the e\n",
      " help or even cure Whitehouse said Whitehouse also praised the efforts of Rhode \n"
     ]
    }
   ],
   "source": [
    "whText = nltk.Text(whTokens)\n",
    "\n",
    "whitehouseIndex = nltk.text.ConcordanceIndex(whText) \n",
    "whitehouseIndex.print_concordance('Whitehouse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note* that the `Text` class is for doing rapid, exploratory analysis. It provides an easy interface to many of the operations we want to do, but it does not allow us much control over the particular operations it glosses. When you are doing a more complete analysis, you should be using the module specifically designed for that task instead of the shortcut method `Text` provides, e.g. use  [`collocations` Module](http://www.nltk.org/api/nltk.html#module-nltk.collocations) instead of `.collocations()`.\n",
    "\n",
    "Now that we have gotten this loaded, let's glance at few features we will delve into more deeply later.\n",
    "\n",
    "For example, we can find words that statistically tend to occur together and typically have a composite, idiomatic meaning irreducible to the semantics of its component words. We will do this later with more control over exactly how these are identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can pick a word (or words) and find what words tend to occur around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for_cell on_cells the_cell hope_cell time_cell own_cells joints_cell\n",
      "of_cell embryonic_cells\n"
     ]
    }
   ],
   "source": [
    "whText.common_contexts(['stem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also just count the number of times the word occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whText.count('cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can also plot each time a set of words occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEWCAYAAAC5XZqEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcNElEQVR4nO3de5QcZZ3/8fcHEgKSQMQEFzAhAnJLhJg0Qgh38mOVZcVdo4iAwPG3EQRdDiKLkiWTXVmMCq6IyC8oBjAIq7IrgucHOVwEYgJ0IDcIV002GIREruFO+O4f9UxoZnsuyUw/Pd39eZ3TZ2qqqp/6PlVJf+apqqlRRGBmZpbTJvUuwMzMWo/Dx8zMsnP4mJlZdg4fMzPLzuFjZmbZOXzMzCw7h48ZIOlASY/0QTvLJU3qxfuPk3RLb+voK321XzZiuyFpl9zbtXwcPtaQevsh31FE3BURu/VVe9VImiXpDUkvpddSSRdI2rqijtkRcUQt69gQtdovkkalgFmbXsslnbMR7Zwk6e6+rs9qz+Fjlte3I2IIMBw4GdgPmCtpy3oVJGnTem0bGBoRg4FjgfMkfayOtVhGDh9rKpI2kXSOpCck/UXSf0jaJi37kaRfVqw7Q9KtKhwi6cmKZSMkXS9pdWrnkjR/Z0m3pXlrJM2WNHRD64yI1yLiPuATwPsoguhdP8mnur4n6RlJL0haLGlMWjZL0mWS5qRR1O8k7VhR/+5p2bOSHpH0mYpls9K++K2kl4FDJR0p6aHU1p8knZXW7bhf9pB0h6TnJT0o6RMd2v2hpJtSO/dI2rmH+2Me8CAwpuMySVtLuiodixWSpqbjvAdwGTAhjZ6e7/EBsLpz+Fiz+QrwSeBgYHvgOeCHadlXgb3SB/yBwBeAE6PDM6bSSOBGYAUwCtgBuLZ9MXBBansPYATQtrHFRsRLwBzgwCqLjwAOAnYFhgLHAH+pWH4c8K/AMGAhMDvVv2Vq8xpgW4pRxaWSRle893PA+cAQ4G7gJ8AX06hsDHBbx2IkDQR+A9yS2v0yMFtS5Wm5Y4HpwHuBx9M2upRCdiIwGnigyio/ALYGdqI4rp8HTo6IZcApwLyIGBwRQ7vblvUfDh9rNl8Ezo2IJyPidYpgmCxpQES8AhwPXAT8DPhyRDxZpY2PUoTL1yLi5TRKuRsgIh6PiDkR8XpErE5tHdzLmlcB21SZ/yZFOOwOKCKWRcRTFctviog7Uz/PpRgBjACOApZHxE8j4q2IuB/4FTC54r2/joi5EfF2RLyWtrWnpK0i4rn0no72AwYD34qINyLiNoqQPrZinesj4t6IeIsiDMd20/c1wLPAj4FzIuLWyoXpB4FjgK9HxEsRsRy4EDihm3atn3P4WLPZEfjPdFroeWAZsA54P0BE3Av8gWIE8x+dtDECWJE+QN9F0raSrk2npl6kCLFhvax5B4oP4HdJH+6XUIzcnpY0U9JWFausrFh3bWpje4p9sG/7Pkj74Tjgr6q9N/kUcCSwIp3Cm1Clzu2BlRHxdsW8Fan+dn+umH6FIqy6Miwi3hsRe0TExdWWA5ul7XS2TWtADh9rNiuBj0fE0IrX5hHxJwBJpwGDKEYbZ3fRxkhJA6osuwAIYK+I2IpiJKWNLVbSYGAScFe15RFxcUSMpzgltSvwtYrFIzq0sw1Fv1YCv+uwDwZHxKmVTXfYzn0RcTTF6bT/onowrwJGSKr83BgJ/KlHnd04ayhGZTtWzKvcph/L36AcPtbIBkravOI1gOIC9PntF98lDZd0dJreFfgmRWCcAJwtaWyVdu8FngK+JWnL1PbEtGwIsBZ4XtIOvDsMekzSIEnjKT7onwN+WmWdfSTtm661vAy8RjGKa3ekpAMkbUZx7eeeiFhJcSpsV0knSBqYXvukC/TVatlMxe8XbR0RbwIvdthOu3tSHWenNg8B/pZ3rof1uYhYRxGE50sako7rmRQjToCngQ+kfWANxOFjjey3wKsVrzbg+8ANwC2SXgLmU5yCGkDxgTUjIhZFxGPAN4CrJQ2qbDR94P0tsAvw38CTFNcdoLiYPg54AbgJuH4Daz471fUscBWwANg/Il6usu5WwOUU4bSC4maD71YsvwaYltoaT3Fqrf0mhiOAz1KMVv4MzKAY8XXmBGB5OpV4CkVAv0tEvEFxd97HKUYklwKfj4iHe9LxXvgyRej9geLmiGuAK9Ky2yjukvuzpDU1rsP6kPzH5Mwaj6RZwJMRMbXetZhtDI98zMwsO4ePmZll59NuZmaWnUc+ZmaWXbXfY7AOhg0bFqNGjap3GWZmDWXBggVrImJ4tWUOnx4YNWoU5XK53mWYmTUUSSs6W+bTbmZmlp3Dx8zMsnP4mJlZdg4fMzPLzuFjZmbZOXzMzCw7h4+ZmWXn8DEzs+wcPmZmlp3Dx8zMsnP4mJlZdg4fMzPLzuFjZmbZOXzMzCw7h4+ZmWXn8DEzs+wcPmZmlp3Dx8zMsnP4mJlZdg4fMzPLzuFjZmbZOXzMzCy7hg0fiXMlHpRYLLFQYt9612RmZj3TkOEjMQE4ChgXwV7AJGBlfavqnba2eldgZpZPQ4YPsB2wJoLXASJYE8EqifESv5NYIHGzxHYAEndIzJC4V+JRiQPrWn0V06fXuwIzs3waNXxuAUakILlU4mCJgcAPgMkRjAeuAM6veM+ACD4KnAFMy16xmZmtN6DeBWyMCNZKjAcOBA4FrgO+CYwB5kgAbAo8VfG269PXBcCo7rYhaQowBWDkyJF9VLmZmUGDhg9ABOuAO4A7JJYApwEPRjChk7e8nr6uowf9joiZwEyAUqkUvS7YzMzWa8jTbhK7SXyoYtZYYBkwPN2MgMRAidH1qM/MzLrWkOEDDAaulHhIYjGwJ3AeMBmYIbEIWAjs31UjEttL/LbWxfbENF+FMrMWogifUepOqVSKcrlc7zLMzBqKpAURUaq2rFFHPmZm1sAcPmZmlp3Dx8zMsnP4mJlZdg4fMzPLzuFjZmbZOXzMzCw7h4+ZmWXn8DEzs+wcPmZmlp3Dx8zMsnP4mJlZdg4fMzPLzuFjZmbZOXzMzCw7h4+ZmWXn8DEzs+wcPmZmlp3Dx8zMsmvo8JE4Q+I99a6jntraGrPtVud9u/Fy7bt6HqP+9O+jVrUoImrTcgYSy4FSBGtquZ1SqRTlcrmWm9hoEtTqENay7Vbnfbvxcu27eh6j/vTvoze1SFoQEaVqyxpm5COxpcRNEosklkpMA7YHbpe4Pa1zhMQ8ifslfiExOM1fLvFvaVlZYpzEzRJPSJxSz36ZmbWihgkf4GPAqgj2jmAM8O/AKuDQCA6VGAZMBSZFMA4oA2dWvH9lBBOAu4BZwGRgP+Bfqm1M0hRJZUnl1atX16pPZmYtqZHCZwkwSWKGxIERvNBh+X7AnsBciYXAicCOFctvqGjnngheimA18JrE0I4bi4iZEVGKiNLw4cP7ui9mZi1tQL0L6KkIHpUYDxwJXCBxS4dVBMyJ4NhOmng9fX27Yrr9+4bZD2ZmzaBhRj4S2wOvRPAz4LvAOOAlYEhaZT4wUWKXtP57JHatS7EZTZvWmG23Ou/bjZdr39XzGPWnfx+1qqVh7naT+GvgOxQjlTeBU4EJwGnAU+m6z2HADGBQetvUCG6ovCtO4qQ0fXpqd/2yzrbdn+92MzPrr7q6261hwqeeHD5mZhuuKW61NjOz5uHwMTOz7Bw+ZmaWncPHzMyyc/iYmVl2Dh8zM8vO4WNmZtk5fMzMLDuHj5mZZefwMTOz7Bw+ZmaWncPHzMyyc/iYmVl2Dh8zM8vO4WNmZtk5fMzMLDuHj5mZZefwMTOz7FoyfCROkrgkTbdJnFXvmnqjra0x2+6vGrHPjVhzb7Raf5uRIqLeNWQncRJQiuB0iTZgbQTf7Wz9UqkU5XI5V3kbTIJaHcZatt1fNWKfG7Hm3mi1/jYqSQsiolRtWVONfCQ+L7FYYpHE1RLDJX4lcV96Tax3jWZmBgPqXUBfkRgNnAtMjGCNxDbAJcD3IrhbYiRwM7BHz9rTFGAKwMiRI2tUtZlZa2qa8AEOA34ZwRqACJ6VmATsKa1fZyuJIT1pLCJmAjOhOO3W9+WambWuZgofAR1DYhNgQgSvvmtFYWZmddRM13xuBT4j8T6AdNrtFuD09hUkxtantNqaNq0x2+6vGrHPjVhzb7Raf5tRU93tJnEi8DVgHfAAcBbwQ4rrPAOAOyM4pdnudjMz64+6ututqcKnVhw+ZmYbrmVutTYzs8bg8DEzs+wcPmZmlp3Dx8zMsnP4mJlZdg4fMzPLzuFjZmbZOXzMzCw7h4+ZmWXn8DEzs+wcPmZmlp3Dx8zMsnP4mJlZdg4fMzPLzuFjZmbZOXzMzCw7h4+ZmWXn8DEzs+zqHj4SX5FYJjG7h+vfIVH1z7JuxLbbJM7qi7aaQVtbvSsws1ZR9/ABvgQcGcFxtWhcYtNatNuMpk+vdwVm1ioG1HPjEpcBOwE3SFwL7Ax8ONXVFsGvJbYAfgrsCSwDtqh4/xHAdGAQ8ARwcgRrJZYDVwBHAJdIDAGmAJsBjwMnRPBKnl6amVlHdR35RHAKsAo4FNgSuC2CfdL335HYEjgVeCWCvYDzgfEAEsOAqcCkCMYBZeDMiuZfi+CACK4Fro9gnwj2pgiwL3RXm6QpksqSyqtXr+6rLpuZGXUe+XRwBPCJimswmwMjgYOAiwEiWCyxOC3fj2I0NFcCilHNvIr2rquYHiPxTWAoMBi4ubtiImImMBOgVCrFRvXIzMyq6k/hI+BTETzyrplFsFT78BcwJ4JjO2nv5YrpWcAnI1gkcRJwSC9rNTOzXugPNxy0uxn4soQAJD6S5t8Jxc0IEmOAvdL8+cBEiV3SsvdI7NpJ20OApyQGtrdl/9u0afWuwMxaRX8Kn38FBgKLJZam7wF+BAxOp9vOBu4FiGA1cBLw87RsPrB7J23/M3APMAd4uFYdaHS+1drMclGEL2d0p1QqRblcrncZZmYNRdKCiKj6e5n9aeRjZmYtwuFjZmbZOXzMzCw7h4+ZmWXn8DEzs+wcPmZmlp3Dx8zMsnP4mJlZdg4fMzPLzuFjZmbZOXzMzCw7h4+ZmWXn8DEzs+wcPmZmlp3Dx8zMsnP4mJlZdg4fMzPLzuFjZmbZNU34SKytMu8Uic+n6VkSk/NXBm1t9dhqbTRTX/pKf94n/bk2a22KiHrX0Cck1kYwuIvls4AbI/jlhrZdKpWiXC73pjaaZDc3VV/6Sn/eJ/25Nmt+khZERKnasqYZ+VQj0SZxVpX550ncJ7FUYqaE6lGfmVmraurw6cIlEewTwRhgC+CojitImiKpLKm8evXq/BWamTWxVg2fQyXukVgCHAaM7rhCRMyMiFJElIYPH56/QjOzJjag3gXkJrE5cClQimClRBuweX2rMjNrLa048mkPmjUSg6H2d8BNm1brLeTTTH3pK/15n/Tn2qy1NdPdbm8DqypmXQRsBayN4LuVd7tJfBP4LLAcWAmsiKCts7Z7e7ebmVkr6uput6Y57RbR9SgugpMqpqcCU2tdk5mZVdeKp93MzKzOHD5mZpadw8fMzLJz+JiZWXYOHzMzy87hY2Zm2Tl8zMwsO4ePmZll5/AxM7PsHD5mZpadw8fMzLJz+JiZWXYOHzMzy87hY2Zm2Tl8zMwsO4ePmZll5/AxM7PsHD5mZpZdvwkfiXUSCyUWSdwvsf9GtLG2FrX1hba2eldgZtZ/9JvwAV6NYGwEewNfBy7o6RslJPWuLxIDevP+7kyfXsvWzcwaS38Kn0pbAc8BSAyWuDWNhpZIHJ3mj5JYJnEpcD8wIs2/MK17q8TwNG9nif8vsUDiLond0/xZEhdJ3A7MqEdHzcxaUU1/2t9AW0gsBDYHtgMOS/NfA/4ughclhgHzJW5Iy3YDTo7gSwASWwL3R/BVifOAacDpwEzglAgek9gXuLSi/V2BSRGsqyxG0hRgCsDIkSNr0mEzs1bVn8Ln1QjGAkhMAK6SGAMI+DeJg4C3gR2A96f3rIhgfkUbbwPXpemfAddLDAb2B34hrV9vUMV7ftExeAAiYiZFaFEqlaLXvTMzs/X6U/isF8G8NMoZDhyZvo6P4E2J5RSjI4CXu2uK4tTi8+3BVkV3bZiZWR/rl9d80jWZTYG/AFsDz6TgORTYsYu3bgJMTtOfA+6O4EXgjxKfTm1LYu/aVV/dtGm5t2hm1n/1p5FP+zUfKE61nRjBOonZwG8kysBC4OEu2ngZGC2xAHgBOCbNPw74kcRUYCBwLbCo77vQOd9qbWb2DkX4ckZ3SqVSlMvlepdhZtZQJC2IiFK1Zf3ytJuZmTU3h4+ZmWXn8DEzs+wcPmZmlp3Dx8zMsnP4mJlZdg4fMzPLzuFjZmbZOXzMzCw7h4+ZmWXn8DEzs+wcPmZmlp3Dx8zMsnP4mJlZdg4fMzPLzuFjZmbZOXzMzCw7h4+ZmWXXbfhIfE/ijIrvb5b4ccX3F0qcKXFjJ+//scSeafobPSlKYm1P1jMzs8bUk5HP74H9ASQ2AYYBoyuW7w8M7OzNEfzfCB5K3/YofFpZW1u9K6iuv9ZVS63Y51pq35/erwagiOh6BbE9cG8EH5D4MHAWsB1wDPAK8DTwGYpgWQOMARYAx0cQEnek90wGvgYsAR6M4DiJ44GvAJsB9wBfimBdGvl8HzgKeBU4OoKnJXYErgCGA6uBkyP4b4lZwI0R/DLVvDaCwRLbAdcBWwEDgFMjuEviCGA6MAh4IrXT6WirVCpFuVzu4S7tHQm6OSR10V/rqqVW7HMtte9P79fWIWlBRJSqLet25BPBKuAtiZEUo5x5FEExASgBi4E3gI8AZwB7AjsBEzu0cw7wagRjU/DsQRFgEyMYC6wDjkurbwnMj2Bv4E7gH9L8S4CrItgLmA1c3E35nwNuTu3vDSyUGAZMBSZFMA4oA2d2tx/MzKzvDOjhenMpgmd/4CJghzT9AsVpOShGR08CSCwERgF3d9Hm4cB44D4JgC2AZ9KyN2D9NaQFwP9J0xOAv0/TVwPf7qbu+4ArJAYC/xXBQomDKQJybtruZhSB+i6SpgBTAEaOHNnNZszMbEP0NHzar/t8GFgKrAS+CrxIcRoM4PWK9df1oG0BV0bw9SrL3oygfWDeVVvt67xFGsVJiCJQiOBOiYOAvwGulvgO8BwwJ4JjuyouImYCM6E47dZNX8zMbAP09FbruRTXX56NYF0EzwJDKUYi/2vU0IU30ygE4FZgssS2ABLbpGs6Xfk98Nk0fRzvjKyWU4yiAI4m3QCR2nsmgsuBnwDjgPnARIld0jrvkdh1A/pgZma91NPwWUJxl9v8DvNeiGDNBmxvJrBYYna6A24qcIvEYmAOxY0MXfkKcHJa/wTgH9P8y4GDJe4F9gVeTvMPobjO8wDwKeD7EawGTgJ+ntqZD+y+AX2oqWnT6l1Bdf21rlpqxT7XUvv+9H416MHdbpb3bjczs2bRq7vdzMzM+prDx8zMsnP4mJlZdg4fMzPLzuFjZmbZOXzMzCw7h4+ZmWXn8DEzs+wcPmZmlp3Dx8zMsnP4mJlZdg4fMzPLzuFjZmbZOXzMzCw7h4+ZmWXn8DEzs+wcPmZmlp3Dx8zMsnP4mJlZdg4fMzPLzuFjZmbZOXzMzCw7RUS9a+j3JK0GVtS7jj4yDFhT7yL6WDP2CZqzX+5T4+iLfu0YEcOrLXD4tBhJ5Ygo1buOvtSMfYLm7Jf71Dhq3S+fdjMzs+wcPmZmlp3Dp/XMrHcBNdCMfYLm7Jf71Dhq2i9f8zEzs+w88jEzs+wcPmZmlp3Dp4lJWi5piaSFkspp3jaS5kh6LH19b73r7I6kKyQ9I2lpxbxO+yHp65Iel/SIpL+uT9Vd66RPbZL+lI7XQklHVixrhD6NkHS7pGWSHpT0j2l+ox+rzvrVsMdL0uaS7pW0KPVpepqf71hFhF9N+gKWA8M6zPs2cE6aPgeYUe86e9CPg4BxwNLu+gHsCSwCBgEfBJ4ANq13H3rYpzbgrCrrNkqftgPGpekhwKOp9kY/Vp31q2GPFyBgcJoeCNwD7JfzWHnk03qOBq5M01cCn6xfKT0TEXcCz3aY3Vk/jgaujYjXI+KPwOPAR3PUuSE66VNnGqVPT0XE/Wn6JWAZsAONf6w661dn+n2/orA2fTswvYKMx8rh09wCuEXSAklT0rz3R8RTUPynAratW3W901k/dgBWVqz3JF1/UPQ3p0tanE7LtZ/yaLg+SRoFfITiJ+qmOVYd+gUNfLwkbSppIfAMMCcish4rh09zmxgR44CPA6dJOqjeBWWgKvMa5fcJfgTsDIwFngIuTPMbqk+SBgO/As6IiBe7WrXKvEbqV0Mfr4hYFxFjgQ8AH5U0povV+7xPDp8mFhGr0tdngP+kGCY/LWk7gPT1mfpV2Cud9eNJYETFeh8AVmWubaNExNPpA+Ft4HLeOa3RMH2SNJDiA3p2RFyfZjf8sarWr2Y4XgAR8TxwB/AxMh4rh0+TkrSlpCHt08ARwFLgBuDEtNqJwK/rU2GvddaPG4DPShok6YPAh4B761DfBmv/T5/8HcXxggbpkyQBPwGWRcRFFYsa+lh11q9GPl6Shksamqa3ACYBD5PzWNX7rgu/avMCdqK4O2UR8CBwbpr/PuBW4LH0dZt619qDvvyc4rTGmxQ/gX2hq34A51LcjfMI8PF6178BfboaWAIsTv/Zt2uwPh1AcSpmMbAwvY5sgmPVWb8a9ngBewEPpNqXAuel+dmOlR+vY2Zm2fm0m5mZZefwMTOz7Bw+ZmaWncPHzMyyc/iYmVl2Dh+zPiLpe5LOqPj+Zkk/rvj+QklnbmTbh0i6sZNlB6QnFD+cXlMqlg2XdI+kByQdKOnT6enMt29EDd/YmNrNqnH4mPWd3wP7A0jaBBgGjK5Yvj8wtycNSdq0h+v9FXANcEpE7E7xOylflPQ3aZXDgYcj4iMRcRfF7xN9KSIO7Un7HTh8rM84fMz6zlxS+FCEzlLgJUnvlTQI2AN4QNLhaSSyJD2QchCs//tL50m6G/i0pI+lkczdwN93ss3TgFnxzlOX1wBnA+dIGkvxiPwj09+bmUYRTpdJ+o6k0WnEtDA9HPNDqY7jK+b/v/QAym8BW6R5s/t+11mrGVDvAsyaRUSskvSWpJEUITSP4sm/E4AXKH6bfBNgFnB4RDwq6SrgVODfUzOvRcQBkjan+C3zwygeX39dJ5sdzTuPwG9XBkZHxEJJ5wGliDgdQNKhFH+DpizpB8D3I2K2pM2ATSXtARxD8VDaNyVdChwXEedIOj2KB1Ga9ZpHPmZ9q3300x4+8yq+/z2wG/DHiHg0rX8lxR+Wa9ceMrun9R6L4jEkP+tke6L604V78uiSecA3JP0TsGNEvEpxmm48cF963P7hFI9qMutTDh+zvtV+3efDFKfd5lOMfNqv91R7NH2llyumexIgDwKlDvPGAw9198aIuAb4BPAqcLOkw1J9V0bE2PTaLSLaelCH2QZx+Jj1rbnAUcCzUTxu/1lgKEUAzaN4cvAoSbuk9U8AflelnYeBD0raOX1/bCfb+yFwUrq+g6T3ATMorvV0SdJOwB8i4mKKB2PuRfEwycmStk3rbCNpx/SWN9OfFjDrNYePWd9aQnGX2/wO816IiDUR8RpwMvALSUuAt4HLOjaS1psC3JRuOFhRbWNR/LXJ44HLJT1MMfK6IiJ+04NajwGWptNruwNXRcRDwFSKv4C7GJgDtP/pgJnAYt9wYH3BT7U2M7PsPPIxM7PsHD5mZpadw8fMzLJz+JiZWXYOHzMzy87hY2Zm2Tl8zMwsu/8BI7eXG581K4UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.reset_orig() #Seaborn messes with this plot, disabling it\n",
    "whText.dispersion_plot(['Sen.','stem', 'cell', 'federal' ,'Lila', 'Barber', 'Whitehouse'])\n",
    "sns.set() #Re-enabling seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to do an analysis of all the Whitehouse press releases we will first need to obtain them. By looking at the API we can see the the URL we want is [https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/raw/Whitehouse](https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/raw/Whitehouse), so we can create a function to scrape the individual files.\n",
    "\n",
    "If you want to know more about downloading from APIs, refer back to the 1st notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>path</th>\n",
       "      <th>html_url</th>\n",
       "      <th>download_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10Apr2007Whitehouse123.txt</td>\n",
       "      <td>SEN. WHITEHOUSE SHARES WESTERLY GIRL'S STORY I...</td>\n",
       "      <td>raw/Whitehouse/10Apr2007Whitehouse123.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10Apr2008Whitehouse2.txt</td>\n",
       "      <td>SEN. WHITEHOUSE SAYS PRESIDENT BUSH MUST BEGIN...</td>\n",
       "      <td>raw/Whitehouse/10Apr2008Whitehouse2.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10Apr2008Whitehouse3.txt</td>\n",
       "      <td>EPA MUST REVIEW LEGAL PROCESS TO ROOT OUT POLI...</td>\n",
       "      <td>raw/Whitehouse/10Apr2008Whitehouse3.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10Aug2007Whitehouse78.txt</td>\n",
       "      <td>R.I. SENATORS PRAISE SEN. DENIAL OF LNG FACILI...</td>\n",
       "      <td>raw/Whitehouse/10Aug2007Whitehouse78.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10Jan2008Whitehouse35.txt</td>\n",
       "      <td>SEN. WHITEHOUSE COMMENTS ON ONE-YEAR ANNIVERSA...</td>\n",
       "      <td>raw/Whitehouse/10Jan2008Whitehouse35.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  \\\n",
       "0  10Apr2007Whitehouse123.txt   \n",
       "1    10Apr2008Whitehouse2.txt   \n",
       "2    10Apr2008Whitehouse3.txt   \n",
       "3   10Aug2007Whitehouse78.txt   \n",
       "4   10Jan2008Whitehouse35.txt   \n",
       "\n",
       "                                                text  \\\n",
       "0  SEN. WHITEHOUSE SHARES WESTERLY GIRL'S STORY I...   \n",
       "1  SEN. WHITEHOUSE SAYS PRESIDENT BUSH MUST BEGIN...   \n",
       "2  EPA MUST REVIEW LEGAL PROCESS TO ROOT OUT POLI...   \n",
       "3  R.I. SENATORS PRAISE SEN. DENIAL OF LNG FACILI...   \n",
       "4  SEN. WHITEHOUSE COMMENTS ON ONE-YEAR ANNIVERSA...   \n",
       "\n",
       "                                        path  \\\n",
       "0  raw/Whitehouse/10Apr2007Whitehouse123.txt   \n",
       "1    raw/Whitehouse/10Apr2008Whitehouse2.txt   \n",
       "2    raw/Whitehouse/10Apr2008Whitehouse3.txt   \n",
       "3   raw/Whitehouse/10Aug2007Whitehouse78.txt   \n",
       "4   raw/Whitehouse/10Jan2008Whitehouse35.txt   \n",
       "\n",
       "                                            html_url  \\\n",
       "0  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "1  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "2  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "3  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "4  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "\n",
       "                                        download_url  \n",
       "0  https://raw.githubusercontent.com/lintool/Grim...  \n",
       "1  https://raw.githubusercontent.com/lintool/Grim...  \n",
       "2  https://raw.githubusercontent.com/lintool/Grim...  \n",
       "3  https://raw.githubusercontent.com/lintool/Grim...  \n",
       "4  https://raw.githubusercontent.com/lintool/Grim...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getGithubFiles(target, maxFiles = 100):\n",
    "    #We are setting a max so our examples don't take too long to run\n",
    "    #For converting to a DataFrame\n",
    "    releasesDict = {\n",
    "        'name' : [], #The name of the file\n",
    "        'text' : [], #The text of the file, watch out for binary files\n",
    "        'path' : [], #The path in the git repo to the file\n",
    "        'html_url' : [], #The url to see the file on Github\n",
    "        'download_url' : [], #The url to download the file\n",
    "    }\n",
    "\n",
    "    #Get the directory information from Github\n",
    "    r = requests.get(target)\n",
    "    filesLst = json.loads(r.text)\n",
    "\n",
    "    for fileDict in filesLst[:maxFiles]:\n",
    "        #These are provided by the directory\n",
    "        releasesDict['name'].append(fileDict['name'])\n",
    "        releasesDict['path'].append(fileDict['path'])\n",
    "        releasesDict['html_url'].append(fileDict['html_url'])\n",
    "        releasesDict['download_url'].append(fileDict['download_url'])\n",
    "\n",
    "        #We need to download the text though\n",
    "        text = requests.get(fileDict['download_url']).text\n",
    "        releasesDict['text'].append(text)\n",
    "\n",
    "    return pandas.DataFrame(releasesDict)\n",
    "\n",
    "whReleases = getGithubFiles('https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/raw/Whitehouse', maxFiles = 10)\n",
    "whReleases[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are having trouble downloading the data uncomment this next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whReleases = pandas.read_csv('../data/whReleases.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the texts in a DataFrame we can look at a few things.\n",
    "\n",
    "First let's tokenize the texts with the same tokenizer as we used before. We will just save the tokens as a list for now; no need to convert to `Text`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "whReleases['tokenized_text'] = whReleases['text'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how long each of the press releases is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    350\n",
       "1    311\n",
       "2    497\n",
       "3    189\n",
       "4    236\n",
       "5    334\n",
       "6    241\n",
       "7    469\n",
       "8    443\n",
       "9    425\n",
       "Name: word_counts, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whReleases['word_counts'] = whReleases['tokenized_text'].apply(lambda x: len(x))\n",
    "whReleases['word_counts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that input and interrogate a corpus relating to your anticipated final project. This could include one of the Davies corpora or some other you have constructed. Turn your text into an nltk `Text` object, and explore all of the features examined above, and others that relate to better understanding your corpus in relation to your research question. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and normalizing text\n",
    "\n",
    "As we want to start comparing the different releases we may choose to do a bit of filtering and normalizing that will allow us to focus on what we most care about. We can first make all of the words lower case, then drop the non-word tokens. Next, we can remove some 'stop words', stem the remaining words to remove suffixes, prefixes and (in some languages) infixes, or lemmatize tokens by intelligently grouping inflected or variant forms of the same word (e.g., with a stemmer and a dictionary). \n",
    "\n",
    "To begin this process, we will first define a function to work over the tokenized lists, then another to add normalized tokens to a new column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using spaCy's built in stop words identifying capacity. When we run text through spaCy's language pipeline, it automatically tags it as a stop word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove these stopwords from the analysis by fiat, but we could also take a more principled approach by looking at the frequency distribution of words and selecting a specific cut-off associated with the preservation of 'meaningful words' identified upon inspection. Alternatively, we could automatically set a cut-off by rule, such as removal of all words more frequent then the most frequent verb, or the most frequent noun (not pronoun), or some term of central interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 179),\n",
       " ('of', 112),\n",
       " ('to', 112),\n",
       " ('and', 108),\n",
       " ('in', 64),\n",
       " ('a', 61),\n",
       " (\"'s\", 50),\n",
       " ('that', 41),\n",
       " ('our', 39),\n",
       " ('for', 35),\n",
       " ('on', 33),\n",
       " ('is', 33),\n",
       " ('Whitehouse', 31),\n",
       " ('Iraq', 28),\n",
       " ('Bush', 27),\n",
       " ('President', 25),\n",
       " ('troops', 21),\n",
       " ('by', 19),\n",
       " ('American', 19),\n",
       " ('with', 18)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countsDict = {}\n",
    "for word in whReleases['tokenized_text'].sum():\n",
    "    if word in countsDict:\n",
    "        countsDict[word] += 1\n",
    "    else:\n",
    "        countsDict[word] = 1\n",
    "word_counts = sorted(countsDict.items(), key = lambda x : x[1], reverse = True)\n",
    "word_counts[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at this and pick the cutoff, often at the first noun. So we will cut all words before `'Whitehouse'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'of', 'to', 'and', 'in', 'a', \"'s\", 'that', 'our', 'for', 'on', 'is']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The stop list is then all words that occur before the first noun\n",
    "stop_words_freq = []\n",
    "for word, count in word_counts:\n",
    "    if word == 'Whitehouse':\n",
    "        break\n",
    "    else:\n",
    "        stop_words_freq.append(word)\n",
    "stop_words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we did have any extra words to add as stop words apart from those normally considered as stop words, we would add it to our stop words by passing it to the normalize tokens method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our function to normalize the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeTokens(word_list, extra_stop=[]):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "    normalized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "\n",
    "    doc = nlp(word_list.lower())\n",
    "    \n",
    "    # add the property of stop word to words considered as stop words\n",
    "    if len(extra_stop) > 0:\n",
    "        for stopword in extra_stop:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    for w in doc:\n",
    "        # if it's not a stop word or punctuation mark, add it to our article\n",
    "        if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "            normalized.append(str(w.lemma_))\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>path</th>\n",
       "      <th>html_url</th>\n",
       "      <th>download_url</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>normalized_tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10Apr2007Whitehouse123.txt</td>\n",
       "      <td>SEN. WHITEHOUSE SHARES WESTERLY GIRL'S STORY I...</td>\n",
       "      <td>raw/Whitehouse/10Apr2007Whitehouse123.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>[SEN, WHITEHOUSE, SHARES, WESTERLY, GIRL, 'S, ...</td>\n",
       "      <td>350</td>\n",
       "      <td>[sen, whitehouse, share, westerly, girl, story...</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10Apr2008Whitehouse2.txt</td>\n",
       "      <td>SEN. WHITEHOUSE SAYS PRESIDENT BUSH MUST BEGIN...</td>\n",
       "      <td>raw/Whitehouse/10Apr2008Whitehouse2.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>[SEN, WHITEHOUSE, SAYS, PRESIDENT, BUSH, MUST,...</td>\n",
       "      <td>311</td>\n",
       "      <td>[sen, whitehouse, say, president, bush, begin,...</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10Apr2008Whitehouse3.txt</td>\n",
       "      <td>EPA MUST REVIEW LEGAL PROCESS TO ROOT OUT POLI...</td>\n",
       "      <td>raw/Whitehouse/10Apr2008Whitehouse3.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>[EPA, MUST, REVIEW, LEGAL, PROCESS, TO, ROOT, ...</td>\n",
       "      <td>497</td>\n",
       "      <td>[epa, review, legal, process, root, political,...</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10Aug2007Whitehouse78.txt</td>\n",
       "      <td>R.I. SENATORS PRAISE SEN. DENIAL OF LNG FACILI...</td>\n",
       "      <td>raw/Whitehouse/10Aug2007Whitehouse78.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>[R.I., SENATORS, PRAISE, SEN, DENIAL, OF, LNG,...</td>\n",
       "      <td>189</td>\n",
       "      <td>[r.i, senator, praise, sen, denial, lng, facil...</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10Jan2008Whitehouse35.txt</td>\n",
       "      <td>SEN. WHITEHOUSE COMMENTS ON ONE-YEAR ANNIVERSA...</td>\n",
       "      <td>raw/Whitehouse/10Jan2008Whitehouse35.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>[SEN, WHITEHOUSE, COMMENTS, ON, ONE, YEAR, ANN...</td>\n",
       "      <td>236</td>\n",
       "      <td>[sen, whitehouse, comment, year, anniversary, ...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10Mar2008Whitehouse8.txt</td>\n",
       "      <td>SENS. REED, WHITEHOUSE WELCOME RHODE ISLAND ST...</td>\n",
       "      <td>raw/Whitehouse/10Mar2008Whitehouse8.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>[SENS, REED, WHITEHOUSE, WELCOME, RHODE, ISLAN...</td>\n",
       "      <td>334</td>\n",
       "      <td>[sen, reed, whitehouse, welcome, rhode, island...</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10Sep2007Whitehouse72.txt</td>\n",
       "      <td>REP. WHITEHOUSE ISSUES STATEMENT ON GEN. PETRA...</td>\n",
       "      <td>raw/Whitehouse/10Sep2007Whitehouse72.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>[REP, WHITEHOUSE, ISSUES, STATEMENT, ON, GEN, ...</td>\n",
       "      <td>241</td>\n",
       "      <td>[rep, whitehouse, issue, statement, gen, petra...</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11Apr2007Whitehouse122.txt</td>\n",
       "      <td>SEN. WHITEHOUSE URGES BUSH FOR NEW DIRECTION I...</td>\n",
       "      <td>raw/Whitehouse/11Apr2007Whitehouse122.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>[SEN, WHITEHOUSE, URGES, BUSH, FOR, NEW, DIREC...</td>\n",
       "      <td>469</td>\n",
       "      <td>[sen, whitehouse, urge, bush, new, direction, ...</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11Jan2007Whitehouse161.txt</td>\n",
       "      <td>SENS. REED, WHITEHOUSE URGE PORTUGAL TO RECONS...</td>\n",
       "      <td>raw/Whitehouse/11Jan2007Whitehouse161.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>[SENS, REED, WHITEHOUSE, URGE, PORTUGAL, TO, R...</td>\n",
       "      <td>443</td>\n",
       "      <td>[sen, reed, whitehouse, urge, portugal, recons...</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11Mar2008Whitehouse7.txt</td>\n",
       "      <td>WHITEHOUSE UNVEILS 'BUSH DEBT': $7.7 TRILLION ...</td>\n",
       "      <td>raw/Whitehouse/11Mar2008Whitehouse7.txt</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>[WHITEHOUSE, UNVEILS, BUSH, DEBT, $, 7.7, TRIL...</td>\n",
       "      <td>425</td>\n",
       "      <td>[whitehouse, unveils, bush, debt, $, foregone,...</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  \\\n",
       "0  10Apr2007Whitehouse123.txt   \n",
       "1    10Apr2008Whitehouse2.txt   \n",
       "2    10Apr2008Whitehouse3.txt   \n",
       "3   10Aug2007Whitehouse78.txt   \n",
       "4   10Jan2008Whitehouse35.txt   \n",
       "5    10Mar2008Whitehouse8.txt   \n",
       "6   10Sep2007Whitehouse72.txt   \n",
       "7  11Apr2007Whitehouse122.txt   \n",
       "8  11Jan2007Whitehouse161.txt   \n",
       "9    11Mar2008Whitehouse7.txt   \n",
       "\n",
       "                                                text  \\\n",
       "0  SEN. WHITEHOUSE SHARES WESTERLY GIRL'S STORY I...   \n",
       "1  SEN. WHITEHOUSE SAYS PRESIDENT BUSH MUST BEGIN...   \n",
       "2  EPA MUST REVIEW LEGAL PROCESS TO ROOT OUT POLI...   \n",
       "3  R.I. SENATORS PRAISE SEN. DENIAL OF LNG FACILI...   \n",
       "4  SEN. WHITEHOUSE COMMENTS ON ONE-YEAR ANNIVERSA...   \n",
       "5  SENS. REED, WHITEHOUSE WELCOME RHODE ISLAND ST...   \n",
       "6  REP. WHITEHOUSE ISSUES STATEMENT ON GEN. PETRA...   \n",
       "7  SEN. WHITEHOUSE URGES BUSH FOR NEW DIRECTION I...   \n",
       "8  SENS. REED, WHITEHOUSE URGE PORTUGAL TO RECONS...   \n",
       "9  WHITEHOUSE UNVEILS 'BUSH DEBT': $7.7 TRILLION ...   \n",
       "\n",
       "                                        path  \\\n",
       "0  raw/Whitehouse/10Apr2007Whitehouse123.txt   \n",
       "1    raw/Whitehouse/10Apr2008Whitehouse2.txt   \n",
       "2    raw/Whitehouse/10Apr2008Whitehouse3.txt   \n",
       "3   raw/Whitehouse/10Aug2007Whitehouse78.txt   \n",
       "4   raw/Whitehouse/10Jan2008Whitehouse35.txt   \n",
       "5    raw/Whitehouse/10Mar2008Whitehouse8.txt   \n",
       "6   raw/Whitehouse/10Sep2007Whitehouse72.txt   \n",
       "7  raw/Whitehouse/11Apr2007Whitehouse122.txt   \n",
       "8  raw/Whitehouse/11Jan2007Whitehouse161.txt   \n",
       "9    raw/Whitehouse/11Mar2008Whitehouse7.txt   \n",
       "\n",
       "                                            html_url  \\\n",
       "0  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "1  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "2  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "3  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "4  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "5  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "6  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "7  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "8  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "9  https://github.com/lintool/GrimmerSenatePressR...   \n",
       "\n",
       "                                        download_url  \\\n",
       "0  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "1  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "2  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "3  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "4  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "5  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "6  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "7  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "8  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "9  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "\n",
       "                                      tokenized_text  word_counts  \\\n",
       "0  [SEN, WHITEHOUSE, SHARES, WESTERLY, GIRL, 'S, ...          350   \n",
       "1  [SEN, WHITEHOUSE, SAYS, PRESIDENT, BUSH, MUST,...          311   \n",
       "2  [EPA, MUST, REVIEW, LEGAL, PROCESS, TO, ROOT, ...          497   \n",
       "3  [R.I., SENATORS, PRAISE, SEN, DENIAL, OF, LNG,...          189   \n",
       "4  [SEN, WHITEHOUSE, COMMENTS, ON, ONE, YEAR, ANN...          236   \n",
       "5  [SENS, REED, WHITEHOUSE, WELCOME, RHODE, ISLAN...          334   \n",
       "6  [REP, WHITEHOUSE, ISSUES, STATEMENT, ON, GEN, ...          241   \n",
       "7  [SEN, WHITEHOUSE, URGES, BUSH, FOR, NEW, DIREC...          469   \n",
       "8  [SENS, REED, WHITEHOUSE, URGE, PORTUGAL, TO, R...          443   \n",
       "9  [WHITEHOUSE, UNVEILS, BUSH, DEBT, $, 7.7, TRIL...          425   \n",
       "\n",
       "                                   normalized_tokens  normalized_tokens_count  \n",
       "0  [sen, whitehouse, share, westerly, girl, story...                      224  \n",
       "1  [sen, whitehouse, say, president, bush, begin,...                      159  \n",
       "2  [epa, review, legal, process, root, political,...                      298  \n",
       "3  [r.i, senator, praise, sen, denial, lng, facil...                      116  \n",
       "4  [sen, whitehouse, comment, year, anniversary, ...                      125  \n",
       "5  [sen, reed, whitehouse, welcome, rhode, island...                      197  \n",
       "6  [rep, whitehouse, issue, statement, gen, petra...                      118  \n",
       "7  [sen, whitehouse, urge, bush, new, direction, ...                      249  \n",
       "8  [sen, reed, whitehouse, urge, portugal, recons...                      237  \n",
       "9  [whitehouse, unveils, bush, debt, $, foregone,...                      241  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whReleases['normalized_tokens'] = whReleases['tokenized_text'].apply(lambda x: normalizeTokens(x))\n",
    "\n",
    "whReleases['normalized_tokens_count'] = whReleases['normalized_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "whReleases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that in the final step of normalizing, we add the lemmatized form of the word. spaCy's built in lemmatizer does this (https://spacy.io/api/lemmatizer). Lemmatization can be thought of as finding the root or the base of the word after removing inflections and other variations of words. Another possible way of finding the roots of words is using NLTK's porting or stemming functionalities, but we will not be getting into them.\n",
    "\n",
    "Now that it is cleaned we start analyzing the dataset. We can start by finding frequency distributions for the dataset. Lets start looking at all the press releases together. The [`ConditionalFreqDist`](http://www.nltk.org/api/nltk.html#nltk.probability.ConditionalProbDist) class reads in an iterable of tuples, the first element is the condition and the second the focal word. For starters, we will use word lengths as the conditions, but tags or clusters will provide more useful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.sum() adds together the lists from each row into a single list\n",
    "whcfdist = nltk.ConditionalFreqDist(((len(w), w) for w in whReleases['normalized_tokens'].sum()))\n",
    "\n",
    "#print the number of words\n",
    "print(whcfdist.N())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can lookup the distributions of different word lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whcfdist[3].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a Conditional Probability Distribution or [`ConditionalProbDist`](http://www.nltk.org/api/nltk.html#nltk.probability.ConditionalProbDist) from the `ConditionalFreqDist`. To do this, however, we need a model for the probability distribution. A simple model is [`ELEProbDist`](http://www.nltk.org/api/nltk.html#nltk.probability.ELEProbDist) which gives the expected likelihood estimate for the probability distribution of the experiment used to generate the observed frequency distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whcpdist = nltk.ConditionalProbDist(whcfdist, nltk.ELEProbDist)\n",
    "\n",
    "#print the most common 2 letter word\n",
    "print(whcpdist[2].max())\n",
    "\n",
    "#And its probability\n",
    "print(whcpdist[2].prob(whcpdist[2].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length is one feature of a word, but there are many more important features we care about. Another critical feature is a word's role in the sentence, or its part of speech (POS). \n",
    "\n",
    "The method below works similarly to ```nltk.pos_text```, and can work with the conditional frequency distributions it provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_pos(word_list):\n",
    "    tags = []\n",
    "    doc = nlp(word_list.lower())\n",
    "    for w in doc:\n",
    "        tags.append((w.text, w.tag_))\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whReleases['normalized_tokens_POS'] = [spacy_pos(t) for t in whReleases['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a new column with the part of speech as a short initialism and the word in a tuple, exactly how the `nltk.ConditionalFreqDist()` function wants them. We can now construct another conditional frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whcfdist_WordtoPOS = nltk.ConditionalFreqDist(whReleases['normalized_tokens_POS'].sum())\n",
    "list(whcfdist_WordtoPOS.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the frequency of each word as each part of speech...which can be uninformative and boring. What we want is the converse; the frequency of each part of speech for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whcfdist_POStoWord = nltk.ConditionalFreqDist((p, w) for w, p in whReleases['normalized_tokens_POS'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now identify and collect all of the superlative adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whcfdist_POStoWord['JJ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or look at the most common nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whcfdist_POStoWord['NN'].most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or plot the base form verbs against their number of occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whcfdist_POStoWord['VB'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then do a similar analysis of the word probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whcpdist_POStoWord = nltk.ConditionalProbDist(whcfdist_POStoWord, nltk.ELEProbDist)\n",
    "\n",
    "#print the most common nouns\n",
    "print(whcpdist_POStoWord['NN'].max())\n",
    "\n",
    "#And its probability\n",
    "print(whcpdist_POStoWord['NN'].prob(whcpdist_POStoWord['NN'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even create a...wait for it...*WORD CLOUD* or Wordl to gaze at and draw mystical, approximate inferences about important nouns and verbs in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = wordcloud.WordCloud(background_color=\"white\", max_words=500, width= 1000, height = 1000, mode ='RGBA', scale=.5).generate(' '.join(whReleases['normalized_tokens'].sum()))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"whitehouse_word_cloud.pdf\", format = 'pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that filter, stem and lemmatize the tokens in your corpus, and then creates plots (with titles and labels) that map the word frequency distribution, word probability distribution, and at least two conditional probability distributions that help us better understand the social and cultural game underlying the production of your corpus. Create a wordl of words (or normalized words) and add a few vague comments about what mysteries are revealed through it.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Davies Corpora\n",
    "\n",
    "Now that we have our basic cleaning down, we can arrange our Davies Corpora. Let us try this with the movies corpora we have already loaded.\n",
    "\n",
    "We'll use a smaller corpus of a 1000 movies for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zfile = zipfile.ZipFile(corpus_name + \"/sources_movies.zip\")\n",
    "source = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in zfile.namelist():\n",
    "    with zfile.open(file) as f:\n",
    "        for line in f:\n",
    "            source.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks dirty because the file is encoded as bytes, but we can certainly see the information there. The file id is also present in the original raw text data: as the first \"word\". Look back at the normalized/tokenized words to confirm that. We're going to use this to create a dataframe with: Fileid, movie name, genre, year, and country.\n",
    "\n",
    "It is advised that you run a similar check of the source file before you do other extraction.\n",
    "\n",
    "First, let us create a dictionary mapping file-id to all the text. Each movie will be mapped to a list of the tokenized words.\n",
    "\n",
    "In this example, I only use it to load 100 movies. You can comment this out or increase/decrease the number as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_texts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in movie_raw:\n",
    "    if len(movie_texts) > 100:\n",
    "        break\n",
    "    movies = clean_raw_text(movie_raw[files][1:])\n",
    "    for movie in movies:\n",
    "        txts = lucem_illud_2020.word_tokenize(movie)\n",
    "        try:\n",
    "            movie_texts[txts[0][2:]] = txts[1:]\n",
    "        except IndexError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = pd.DataFrame(columns=[\"Movie Name\", \"Genre\", \"Year\", \"Country\", \"Tokenized Texts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in source[3:]:\n",
    "    try:\n",
    "        tid, fileid, total_words, genre, year, lang, country, imdb, title = movie.decode(\"utf-8\").split(\"\\t\")\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "    try:\n",
    "        movie_df.loc[fileid.strip()] = [title.strip(), genre.strip(), year.strip(), country.strip(), movie_texts[fileid.strip()]]\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In POS tagging, we classify each word by its semantic role in a sentence. We will be using the in-built spaCy pos-tagging.\n",
    "\n",
    "https://spacy.io/usage/linguistic-features\n",
    "\n",
    "|#. |Tag |Description |\n",
    "|---|----|------------|\n",
    "|1.\t|CC\t|Coordinating conjunction\n",
    "|2.\t|CD\t|Cardinal number\n",
    "|3.\t|DT\t|Determiner\n",
    "|4.\t|EX\t|Existential there\n",
    "|5.\t|FW\t|Foreign word\n",
    "|6.\t|IN\t|Preposition or subordinating conjunction\n",
    "|7.\t|JJ\t|Adjective\n",
    "|8.\t|JJR|\tAdjective, comparative\n",
    "|9.\t|JJS|\tAdjective, superlative\n",
    "|10.|\tLS\t|List item marker\n",
    "|11.|\tMD\t|Modal\n",
    "|12.|\tNN\t|Noun, singular or mass\n",
    "|13.|\tNNS\t|Noun, plural\n",
    "|14.|\tNNP\t|Proper noun, singular\n",
    "|15.|\tNNPS|\tProper noun, plural\n",
    "|16.|\tPDT\t|Predeterminer\n",
    "|17.|\tPOS\t|Possessive ending\n",
    "|18.|\tPRP\t|Personal pronoun\n",
    "|19.|\tPRP\\$|\tPossessive pronoun\n",
    "|20.|\tRB\t|Adverb\n",
    "|21.|\tRBR\t|Adverb, comparative\n",
    "|22.|\tRBS\t|Adverb, superlative\n",
    "|23.|\tRP\t|Particle\n",
    "|24.|\tSYM\t|Symbol\n",
    "|25.|\tTO\t|to\n",
    "|26.|\tUH\t|Interjection\n",
    "|27.|\tVB\t|Verb, base form\n",
    "|28.|\tVBD\t|Verb, past tense\n",
    "|29.|\tVBG\t|Verb, gerund or present participle\n",
    "|30.|\tVBN\t|Verb, past participle\n",
    "|31.|\tVBP\t|Verb, non-3rd person singular present\n",
    "|32.|\tVBZ\t|Verb, 3rd person singular present\n",
    "|33.|\tWDT\t|Wh-determiner\n",
    "|34.|\tWP\t|Wh-pronoun\n",
    "|35.|\tWP$\t|Possessive wh-pronoun\n",
    "|36.|\tWRB\t|Wh-adverb\n",
    "\n",
    "spaCy pos-tags word as we run it through the english language model. A small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nlp(u\"Tom went to IKEA to get some of those delicious Swedish meatballs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sent:\n",
    "    print(token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite good. Now we will try POS tagging with a somewhat larger corpus. We consider a few of the top posts from the reddit data we used last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditDF = pandas.read_csv('../data/reddit.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabbing the 10 highest scoring posts and tokenizing the sentences. Once again, notice that we aren't going to do any kind of stemming this week (although *semantic* normalization may be performed where we translate synonyms into the same focal word). Here, we will use sentences tokenized by spaCy. Like before, these methods can also be found in lucem_illud.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize(word_list):\n",
    "    doc = nlp(word_list)\n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditTopScores = redditDF.sort_values('score')[-10:]\n",
    "redditTopScores['sentences'] = redditTopScores['text'].apply(lambda x: [word_tokenize(s) for s in sent_tokenize(x)])\n",
    "redditTopScores.index = range(len(redditTopScores) - 1, -1,-1) #Reindex to make things nice in the future\n",
    "redditTopScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sents_pos(sentences):\n",
    "    \"\"\"\n",
    "    function which replicates NLTK pos tagging on sentences.\n",
    "    \"\"\"\n",
    "    new_sents = []\n",
    "    for sentence in sentences:\n",
    "        new_sent = ' '.join(sentence)\n",
    "        new_sents.append(new_sent)\n",
    "    final_string = ' '.join(new_sents)\n",
    "    doc = nlp(final_string)\n",
    "    \n",
    "    pos_sents = []\n",
    "    for sent in doc.sents:\n",
    "        pos_sent = []\n",
    "        for token in sent:\n",
    "            pos_sent.append((token.text, token.tag_))\n",
    "        pos_sents.append(pos_sent)\n",
    "    \n",
    "    return pos_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditTopScores['POS_sents'] = redditTopScores['sentences'].apply(lambda x: tag_sents_pos(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditTopScores['POS_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And count the number of `NN` (nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countTarget = 'NN'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the adjectives that modify the word, \"computer\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'computer'\n",
    "NResults = set()\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your turn*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform POS tagging on a meaningful (but modest) subset of a corpus associated with your final project. Examine the list of words associated with at least three different parts of speech. Consider conditional associations (e.g., adjectives associated with nouns or adverbs with verbs of interest). What do these distributions suggest about your corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated above, we might want to find significant bigrams (two-word phrases), trigrams (three-word phrases), n-grams (*n*-word phrases) or skip-grams (noncontinguous 'phrases' with skip-length *n*). \n",
    "\n",
    "We will begin with the [`nltk.collocations.BigramCollocationFinder`](http://www.nltk.org/api/nltk.html?highlight=bigramcollocationfinder#nltk.collocations.BigramCollocationFinder) class, which can be given raw lists of strings with the `from_words()` method. By default it only looks at continuous bigrams but there is an option (`window_size`) to allow skip-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whBigrams = nltk.collocations.BigramCollocationFinder.from_words(whReleases['normalized_tokens'].sum())\n",
    "print(\"There are {} bigrams in the finder\".format(whBigrams.N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the bigrams we need to tell nltk what our score function is. Initially, we will look at the raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramScoring(count, wordsTuple, total):\n",
    "    return count\n",
    "\n",
    "print(whBigrams.nbest(bigramScoring, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One note about how `BigramCollocationFinder` works. It doesn't use the strings internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birgramScores = []\n",
    "\n",
    "def bigramPrinting(count, wordsTuple, total):\n",
    "    global birgramScores\n",
    "    birgramScores.append(\"The first word is:  {}, The second word is: {}\".format(*wordsTuple))\n",
    "    #Returns None so all the tuples are considered to have the same rank\n",
    "\n",
    "whBigrams.nbest(bigramPrinting, 10)\n",
    "print('\\n'.join(birgramScores[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words are each given numeric IDs and there is a dictionary that maps the IDs to the words they represent. This is a common performance optimization.\n",
    "\n",
    "Two words can appear together by chance. Recall from  Manning and Schtze's textbook that a t-value can be computed for each bigram to see how significant the association is. You may also want to try computing the $\\chi^2$, likelihood ratio, and pointwise mutual information statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "whBigrams.score_ngrams(bigram_measures.likelihood_ratio)[:40]\n",
    "# other options include student_t, chi_sq, likelihood_ratio, pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few other available measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s for s in dir(bigram_measures) if s[0] != '_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "whTrigrams = nltk.collocations.TrigramCollocationFinder.from_words(whReleases['normalized_tokens'].sum())\n",
    "whTrigrams.score_ngrams(trigram_measures.student_t)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or n-grams (for any number n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ngrams = nltk.ngrams(whReleases['normalized_tokens'].sum(), 4)\n",
    "Counts = {}\n",
    "for ngram in list(Ngrams):\n",
    "    if ngram in Counts.keys():\n",
    "        Counts[ngram] += 1\n",
    "    else:\n",
    "        Counts[ngram] = 1\n",
    "Filtered = {}\n",
    "for key in Counts.keys():\n",
    "    if Counts[key] < 2:\n",
    "        pass\n",
    "    else:\n",
    "        Filtered[key] = Counts[key]\n",
    "print(Filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is another important python text processing package which allows us to use collocations, among other cool methods (which we will explore later weeks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(whReleases['normalized_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [bigram[line] for line in whReleases['normalized_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that identify statistically significant bigrams, trigrams, quadgrams, higher-order *n*grams and skipgrams. Explore whether these collocations are idiomatic and so irreducible to the semantic sum of their component words. You can do this by examination of conditional frequencies (e.g., what else is 'united' besides the 'United States'). If these phrases are idiomatic, what do they suggest about the culture of the world producing them?\n",
    "\n",
    "<span style=\"color:red\">**Stretch**: In Manning and Schtze's textbook, there Section 5.3.2 explores how to use the *t*-test to find words whose co-occurance patterns best distinguish two words. Implement that and use it to explore phrases in your corpus. For instance, you could tell what words come after \"America\" much more often than after \"Iraq\"?</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is also a classification task, which identifies named objects. \n",
    "Like every other part of the pipeline, spaCy also tags words with their tagged entity (https://spacy.io/api/entityrecognizer). You can see the full capacity of what spaCy does by checking out: https://spacy.io/usage/linguistic-features.\n",
    "\n",
    "For training the identification of such entities, spaCy uses a deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on a basic sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nlp(u\"Tom went to IKEA to get some of those delicious Swedish meatballs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sent:\n",
    "    print(token.text, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For only finding ents-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in sent.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sents_ner(sentences):\n",
    "    \"\"\"\n",
    "    function which replicates NLTK ner tagging on sentences.\n",
    "    \"\"\"\n",
    "    new_sents = []\n",
    "    for sentence in sentences:\n",
    "        new_sent = ' '.join(sentence)\n",
    "        new_sents.append(new_sent)\n",
    "    final_string = ' '.join(new_sents)\n",
    "    doc = nlp(final_string)\n",
    "    \n",
    "    pos_sents = []\n",
    "    for sent in doc.sents:\n",
    "        pos_sent = []\n",
    "        for ent in sent.ents:\n",
    "            pos_sent.append((ent.text, ent.label_))\n",
    "        pos_sents.append(pos_sent)\n",
    "    \n",
    "    return pos_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run NER over our entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditTopScores['classified_sents'] = redditTopScores['sentences'].apply(lambda x: tag_sents_ner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditTopScores['classified_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common entities (which are, of course, boring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if ent in entityCounts:\n",
    "                entityCounts[ent] += 1\n",
    "            else:\n",
    "                entityCounts[ent] = 1\n",
    "sortedEntities = sorted(entityCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedEntities[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or those occurring only twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x[0] for x in sortedEntities if x[1] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also list the most common \"non-objects\". (We note that we're not graphing these because there are so few here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonObjCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind == 'ORG':\n",
    "                continue\n",
    "            elif ent in nonObjCounts:\n",
    "                nonObjCounts[ent] += 1\n",
    "            else:\n",
    "                nonObjCounts[ent] = 1\n",
    "sortedNonObj = sorted(nonObjCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedNonObj[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the Organizations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrgCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind == 'ORG':\n",
    "                print(ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These, of course, have much smaller counts.\n",
    "\n",
    "In this particular example we didn't find the most interesting entities - you can remedy that in exercise 4!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform NER on a (modest) subset of your corpus of interest. List all of the different kinds of entities tagged? What does their distribution suggest about the focus of your corpus? For a subset of your corpus, tally at least one type of named entity and calculate the Precision, Recall and F-score for the NER classification just performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing and graph representations\n",
    "\n",
    "Dependency parsing was developed to robustly capture linguistic dependencies from text. The complex tags associated with these parses are detailed [here]('http://universaldependencies.org/u/overview/syntax.html'). When parsing with the dependency parser, we will work directly from the untokenized text. Note that no *processing* takes place before parsing sentences--we do not remove so-called stop words or anything that plays a syntactic role in the sentence, although anaphora resolution and related normalization may be performed before or after parsing to enhance the value of information extraction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using spaCy's built in dependancy parser to go about exploring the tree.\n",
    "Note that unlike previous examples, large corpuses or collections of sentences don't give us as much information, so we will be navigating smaller sentences.\n",
    "\n",
    "Let us start by exploring noun chunks. Noun chunks are base noun phrases  flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun  for example, the lavish green grass or the worlds largest tech fund. To get the noun chunks in a document, simply iterate over the sentences noun chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy uses the terms head and child to describe the words connected by a single arc in the dependency tree. The term dep is used for the arc label, which describes the type of syntactic relation that connects the child to the head. As with other attributes, the value of .dep is a hash value. You can get the string value with .dep_.\n",
    "\n",
    "For your reference:\n",
    "\n",
    "Text: The original token text.\n",
    "\n",
    "Dep: The syntactic relation connecting child to head.\n",
    "\n",
    "Head text: The original text of the token head.\n",
    "\n",
    "Head POS: The part-of-speech tag of the token head.\n",
    "\n",
    "Children: The immediate syntactic dependents of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the syntactic relations form a tree, every word has exactly one head. You can therefore iterate over the arcs in the tree by iterating over the words in the sentence. This is usually the best way to match an arc of interest  from below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import nsubj, VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = set()\n",
    "for possible_subject in doc:\n",
    "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "        verbs.add(possible_subject.head)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can *also* navigate from above. It isn't as efficient though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = []\n",
    "for possible_verb in doc:\n",
    "    if possible_verb.pos == VERB:\n",
    "        for possible_subject in possible_verb.children:\n",
    "            if possible_subject.dep == nsubj:\n",
    "                verbs.append(possible_verb)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating around the local tree\n",
    "A few more convenience attributes are provided for iterating around the local tree from the token. Token.lefts and Token.rights attributes provide sequences of syntactic children that occur before and after the token. Both sequences are in sentence order. There are also two integer-typed attributes, Token.n_lefts and Token.n_rights that give the number of left and right children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"bright red apples on the tree\")\n",
    "print([token.text for token in doc[2].lefts])  # ['bright', 'red']\n",
    "print([token.text for token in doc[2].rights])  # ['on']\n",
    "print(doc[2].n_lefts)  # 2\n",
    "print(doc[2].n_rights)  # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a whole phrase by its syntactic head using the Token.subtree attribute. This returns an ordered sequence of tokens. You can walk up the tree with the Token.ancestors attribute, and check dominance with Token.is_ancestor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Credit and mortgage account holders must submit their requests\")\n",
    "\n",
    "root = [token for token in doc if token.head == token][0]\n",
    "subject = list(root.lefts)[0]\n",
    "for descendant in subject.subtree:\n",
    "    assert subject is descendant or subject.is_ancestor(descendant)\n",
    "    print(descendant.text, descendant.dep_, descendant.n_lefts,\n",
    "            descendant.n_rights,\n",
    "            [ancestor.text for ancestor in descendant.ancestors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the .left_edge and .right_edge attributes can be especially useful, because they give you the first and last token of the subtree. This is the easiest way to create a Span object for a syntactic phrase. Note that .right_edge gives a token within the subtree  so if you use it as the end-point of a range, dont forget to +1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = doc[doc[4].left_edge.i : doc[4].right_edge.i+1]\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(span)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Extraction\n",
    "\n",
    "How can dependancy graphs be useful? We can extract information from the sentences based on relationships between words within parsed phrases. Let us see if we can do this to glean any information, from, say, our movies dataset. \n",
    "\n",
    "How violent are the movies? Who is killing who?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_violences = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in movie_df.iterrows():\n",
    "    text = ' '.join(row['Tokenized Texts'])\n",
    "    movie_violences[row['Movie Name']] = ([], [])\n",
    "    doc = nlp(text)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        subject = 0\n",
    "        object_ = 0\n",
    "        # if the verb or the root of the sentence is kill\n",
    "        if chunk.root.head.text == 'kill':\n",
    "            # we find the subjects and objects around the word, and if it does exist, add it to the tuple\n",
    "            if chunk.root.dep_ == 'nsubj':\n",
    "                subject = chunk.root.text\n",
    "            if chunk.root.dep_ == 'dobj':\n",
    "                object_ = chunk.root.text\n",
    "            if subject is not 0:\n",
    "                movie_violences[row['Movie Name']][0].append(subject)\n",
    "            if object_ is not 0:\n",
    "                movie_violences[row['Movie Name']][1].append(object_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_violences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty neat: we can see who tried to kill who, or at least talked about it. A lot of 'I' killing 'you', as we'd imagine. What else can we see?\n",
    "\n",
    "The movie, about time, which is about time travel, funnily enough has:\n",
    "`'About Time': (['I'], ['Hitler'])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your turn*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, parse a (modest) subset of your corpus of interest. How deep are the phrase structure and dependency parse trees nested? How does parse depth relate to perceived sentence complexity? What are five things you can extract from these parses for subsequent analysis? (e.g., nouns collocated in a noun phrase; adjectives that modify a noun; etc.) Capture these sets of things for a focal set of words (e.g., \"Bush\", \"Obama\", \"Trump\"). What do they reveal about the roles that these entities are perceive to play in the social world inscribed by your texts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional distances\n",
    "\n",
    "If we want to compare different corpora, we need a distance or divergence that compares the two distributions.\n",
    "\n",
    "We will use the: \n",
    "\n",
    "+ Kullback-Leibler (KL) divergence\n",
    "+ $\\chi^2$ divergence\n",
    "+ Kolmogorov-Smirnov (KS) distance\n",
    "+ Wasserstein distance\n",
    "\n",
    "### Kullback-Leibler and $x^2$ divergences ###\n",
    "\n",
    "KL and $\\chi^2$ divergences are members of the broader <a \"href=https://en.wikipedia.org/wiki/F-divergence\" target=\"_blank\">$f$-divergence</a> family, a function of $D_f(P||Q)$ that calculates the difference between two probability distributions P and Q. The KL $f(t)$ is $ t \\text{ log } t $, while the $\\chi^2$ is $t^2-1$. KL comes from information and $\\chi^2$ from measure theory. As such, the KL divergence computes the relative entropy between two distributions--how they differ in bits, while the $\\chi^2$ whether the same statistical inferences can be drawn from them both.  \n",
    "\n",
    "Specifically, given two discrete probability distributions $P$ and $Q$, the Kullback-Leibler divergence from $Q$ to $P$ is defined as:\n",
    "\n",
    "$D_{\\mathrm{KL}}(P\\|Q) = \\sum_i P(i) \\, \\log\\frac{P(i)}{Q(i)}$.\n",
    "\n",
    "The [scipy.stats.entropy()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html) function does the calculation for you, which takes in two arrays of probabilities and computes the KL divergence. Note that the KL divergence is in general not commutative, i.e. $D_{\\mathrm{KL}}(P\\|Q) \\neq D_{\\mathrm{KL}}(Q\\|P)$ .\n",
    "\n",
    "Also note that the KL divernce is the sum of elementwise divergences. Scipy provides [scipy.special.kl_div()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.kl_div.html#scipy-special-kl-div) which calculates elementwise divergences for you.\n",
    "\n",
    "The $\\chi^2$ Divergence is defined as:\n",
    "\n",
    "$D_{\\mathrm{\\chi^2}}(P\\|Q) = \\sum_i \\left(\\frac{P(i)}{Q(i)}-1\\right)^2$. \n",
    "\n",
    "This is also noncommutative, and the code can be drawn directly from scipy.\n",
    "\n",
    "### Kolmogorov-Smirnov ###\n",
    "\n",
    "The two-sample Kolmogovorov-Smirnov test statistic calculates the distance between the cumulative distribution function of the two distributions to be compared, and, along with the $x^2$ divergence, is among the most common approaches two calculating a distance in statistics. It can be interpreted as a test of whether two distributions are drawn from the same underlying distribution. As with the others, the code is readily available in scipy.\n",
    "\n",
    "### Wasserstein Distance ###\n",
    "\n",
    "When this is computed on a Euclidian metric structure (e.g., numbers of words), this is also known as the earth movers distance, because it can be seen as the minimum amount of \"work\" required to transform $P$ into $Q$, where \"work\" is measured as the amount of distribution weight that must be moved, multiplied by the distance it has to be moved.\n",
    "\n",
    "### Computing ###\n",
    "\n",
    "To do this we will need to create the arrays, lets compare the Whitehouse releases with the Kennedy releases. First we have to download them and load them into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kenReleases = getGithubFiles('https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/raw/Kennedy', maxFiles = 10)\n",
    "kenReleases[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can tokenize, stem and remove stop words, like we did for the Whitehouse press releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kenReleases['tokenized_text'] = kenReleases['text'].apply(lambda x: word_tokenize(x))\n",
    "kenReleases['normalized_tokens'] = kenReleases['tokenized_text'].apply(lambda x: normalizeTokens(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to compare the two collection of words, remove those not found in both, and assign the remaining ones indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whWords = set(whReleases['normalized_tokens'].sum())\n",
    "kenWords = set(kenReleases['normalized_tokens'].sum())\n",
    "\n",
    "#Change & to | if you want to keep all words\n",
    "overlapWords = whWords & kenWords\n",
    "\n",
    "overlapWordsDict = {word: index for index, word in enumerate(overlapWords)}\n",
    "overlapWordsDict['student']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can count the occurrences of each word in the corpora and create our arrays. Note that we don't have to use numpy arrays as we do here. We could just use a list, but the arrays are faster in numpy so we encourage you to get in the habit of using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeProbsArray(dfColumn, overlapDict):\n",
    "    words = dfColumn.sum()\n",
    "    countList = [0] * len(overlapDict)\n",
    "    for word in words:\n",
    "        try:\n",
    "            countList[overlapDict[word]] += 1\n",
    "        except KeyError:\n",
    "            #The word is not common so we skip it\n",
    "            pass\n",
    "    countArray = np.array(countList)\n",
    "    return countArray / countArray.sum()\n",
    "\n",
    "whProbArray = makeProbsArray(whReleases['normalized_tokens'], overlapWordsDict)\n",
    "kenProbArray = makeProbsArray(kenReleases['normalized_tokens'], overlapWordsDict)\n",
    "kenProbArray.sum()\n",
    "#There is a little bit of a floating point math error\n",
    "#but it's too small to see with print and too small matter here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the KL divergence. Pay attention to the asymmetry. Use [the JensenShannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence), which is the average KL divergence between each distribution and the average of both distributions (i.e., the midpoint), if you want symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_kenDivergence = scipy.stats.entropy(whProbArray, kenProbArray)\n",
    "print (wh_kenDivergence)\n",
    "ken_whDivergence = scipy.stats.entropy(kenProbArray, whProbArray)\n",
    "print (ken_whDivergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can do the elementwise calculation and see which words best distinguish the two corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_kenDivergence_ew = scipy.special.kl_div(whProbArray, kenProbArray)\n",
    "kl_df = pandas.DataFrame(list(overlapWordsDict.keys()), columns = ['word'], index = list(overlapWordsDict.values()))\n",
    "kl_df = kl_df.sort_index()\n",
    "kl_df['elementwise divergence'] = wh_kenDivergence_ew\n",
    "kl_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_df.sort_values(by='elementwise divergence', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply what we learned today \n",
    "First, let's transform every text into normalized tokens. Note that in this first step, no stopword is removed.\n",
    "We'll use only the first 11 movies: you are welcome to try more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = []\n",
    "for index, row in movie_df.iterrows():\n",
    "    if len(corpora) > 10:\n",
    "        break\n",
    "    corpora.append(row['Tokenized Texts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's separate the normalized tokens into stopwords and non-stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_s = []\n",
    "corpora_nons = []\n",
    "for corpus in corpora:\n",
    "    s = []\n",
    "    nons = []\n",
    "    doc = nlp(' '.join(corpus))\n",
    "    for word in doc:\n",
    "        if word.is_stop:\n",
    "            s.append(word.text)\n",
    "        else:\n",
    "            nons.append(word.text)\n",
    "    corpora_s.append(s)\n",
    "    corpora_nons.append(nons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some covenient funtions for calculating divergence and distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(X, Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    D_kl = scipy.stats.entropy(p, q)\n",
    "    return D_kl\n",
    "\n",
    "def chi2_divergence(X,Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    return scipy.stats.chisquare(p, q).statistic\n",
    "\n",
    "def Divergence(corpus1, corpus2, difference=\"KL\"):\n",
    "    \"\"\"Difference parameter can equal KL, Chi2, or Wass\"\"\"\n",
    "    freqP = nltk.FreqDist(corpus1)\n",
    "    P = pandas.DataFrame(list(freqP.values()), columns = ['frequency'], index = list(freqP.keys()))\n",
    "    freqQ = nltk.FreqDist(corpus2)\n",
    "    Q = pandas.DataFrame(list(freqQ.values()), columns = ['frequency'], index = list(freqQ.keys()))\n",
    "    if difference == \"KL\":\n",
    "        return kl_divergence(P, Q)\n",
    "    elif difference == \"Chi2\":\n",
    "        return chi2_divergence(P, Q)\n",
    "    elif difference == \"KS\":\n",
    "        try:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency']).statistic\n",
    "        except:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency'])\n",
    "    elif difference == \"Wasserstein\":\n",
    "        try:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None).statistic\n",
    "        except:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the KL divergence for each pair of corpora, turn the results into a matrix, and visualize the matrix as a heatmap. Recall that $D_{\\mathrm{KL}}(P\\|Q)$ measures the amount of information loss when $Q$ is used to approximate $P$. Here, the rows are the $P$s used for calculating KL divergences, and the columns are the $Q$s. So, each cell measures the amount of information loss when the word distribution of the column text is used to approximate the word distribution of the row text. Because the KL divergence is directional, such that the divergence of $P$ from $Q$ is different from the same of $Q$ from $P$, the matrix is assymetric and contains unique information above and below the diagonal. The same is true for the $\\chi^2$ divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileids = list(movie_df['Movie Name'])[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for p in corpora:\n",
    "    l = []\n",
    "    for q in corpora:\n",
    "        l.append(Divergence(p,q, difference = 'KL'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To reveal more patterns, let's do a multidimensional scaling of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = sklearn.manifold.MDS()\n",
    "pos = mds.fit(M).embedding_\n",
    "x = pos[:,0]\n",
    "y = pos[:,1]\n",
    "fig, ax = plt.subplots(figsize = (6,6))\n",
    "plt.plot(x, y, ' ')\n",
    "for i, txt in enumerate(fileids):\n",
    "    ax.annotate(txt, (x[i],y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A co-respondent's course and The Chain Gang are away from the others - any guesses why?\n",
    "\n",
    "We may just want to focus on the distrbution of stopwords or non-stopwords. Let's do the analysis again first for stopwords and then for non-stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for p in corpora_s:\n",
    "    l = []\n",
    "    for q in corpora_s:\n",
    "        l.append(Divergence(p,q, difference='KL'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for p in corpora_nons:\n",
    "    l = []\n",
    "    for q in corpora_nons:\n",
    "        l.append(Divergence(p,q, difference='KL'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the same for the assymmetric $\\chi^2$ Divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for p in corpora_nons:\n",
    "    l = []\n",
    "    for q in corpora_nons:\n",
    "        l.append(Divergence(p,q, difference='Chi2'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the KS distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for p in corpora_nons:\n",
    "    l = []\n",
    "    for q in corpora_nons:\n",
    "        l.append(Divergence(p,q, difference='KS'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally for the Wasserstein or \"earth mover's\" Distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for p in corpora_nons:\n",
    "    l = []\n",
    "    for q in corpora_nons:\n",
    "        l.append(Divergence(p,q, difference='Wasserstein'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that decade X is highly divergent and this makes reading the plot somewhat difficult. Let's fix this by taking the log of each cell, which will reduce the distance of decode X from other texts as an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(div.apply(np.log).replace([np.inf, -np.inf], np.nan))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only tried 11 movies, and from the same time period, so our signals might not be too interesting. What about different time periods?\n",
    "\n",
    "An interesting experiment to maybe get some clearer signals: documents by decade in the Davies Corpus Of Historial American English (COHA) and see if we can detect patterns between them. \n",
    "\n",
    "If we want to rerun this on a new data set of our own composition, we can be a bit more efficient with our coding. Let's use the Shakespeare texts from last week as example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_df = lucem_illud_2020.loadTextDirectory('../data/Shakespeare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we stem and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_df['tokenized_text'] = shakespeare_df['text'].apply(lambda x: word_tokenize(x))\n",
    "shakespeare_df['normalized_tokens'] = shakespeare_df['tokenized_text'].apply(lambda x: normalizeTokens(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the corpus file and generate the heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = 'KS' #\"KL\", \"Chi2\", \"KS\", \"Wasserstein\"\n",
    "num_of_texts = 10 #The bigger this number the slower it will run, you can also try selecting your own plays\n",
    "fileids_sp = list(shakespeare_df[:num_of_texts].index)\n",
    "corpora_sp = list(shakespeare_df[:num_of_texts]['normalized_tokens'])\n",
    "L = []\n",
    "for p in corpora_sp:\n",
    "    l = []\n",
    "    for q in corpora_sp:\n",
    "        l.append(Divergence(p,q, difference=measure))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids_sp, index = fileids_sp)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are \"The Passionate Pilgrim\" and \"The Phoenix and the Turtle\"? Little known poems by Shakespeare that are unsurprisingly hard to classify, as they are so different from everything else he wrote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 6*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that calculate the KL and $\\chi^2$ divergences, and the KS and Wasserstein distances between four separate corpora, plot these with heatmaps, and then array them in two dimensions with multidimensional scaling as above. What does this reveal about relations between the corpora? Which analysis (and divergence or distribution) distinguishes the authors or documents better? \n",
    "\n",
    "<span style=\"color:red\">**Stretch**: Calculate the <a \"href=https://en.wikipedia.org/wiki/JensenShannon_divergence\" target=\"_blank\">Jensen-Shannon Divergence</a> between your four corpora. What is the relationship between the KL and JS divergences?</span> "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
